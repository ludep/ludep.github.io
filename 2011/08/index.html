
<!DOCTYPE html>
<html lang="en-US">
<!-- <!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"> -->

<!-- Mirrored from www.ludep.com/2011/08/ by HTTrack Website Copier/3.x [XR&CO'2014], Mon, 15 Mar 2021 00:21:01 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=UTF-8" /><!-- /Added by HTTrack -->
<head>

	<meta charset="UTF-8" />
	<title>August  &#8211;  2011  &#8211;   Leading Units &amp; Drone Enabled Probing</title>
	<link rel="profile" href="http://gmpg.org/xfn/11" />
	<meta name="description" content="Leading Units &amp; Drone Enabled Probing" />	<meta name="keywords" content="August  &#8211;  2011  &#8211;   Leading Units &amp; Drone Enabled Probing" />	

	<link rel="alternate" type="application/atom+xml" title="Atom 0.3" href="../../feed/atom/index.html" />
	<link rel="pingback" href="../../xmlrpc.php" />

	<style type="text/css">/* no_sidebars.css */ div#wrapper { width:100%; }  body { font-size:12px; font-family:Verdana, Geneva, sans-serif; font-weight:normal; color:#505050; background-color:#ffffff; }  /* LAYOUT */ #container { margin: 0 auto; width: 990px; min-width:600px; background-color:#ffffff; } #wrapper { background-color:#ffffff; padding-top:10px; } #content { padding:0 15px 10px 15px; }  .sidebar .column { margin:19px  0 10px 0;}  .sidebar > :first-child h5 { margin-top:0; } #outer-footer { padding-top: 10px } #footer { border-bottom: 4px solid #27a5c4; border-top: 2px solid #27a5c4; background-color: #b7dee8; }  #header { color: #27a5c4; font-family:Georgia, serif; background-color:#ffffff; margin: 0; padding: 0; margin: 0; } #masthead { margin: 0px 0px 0px 10px; padding-left:0px; } #header h1, #header h3 { font-size:64px; margin:0; font-weight:normal; letter-spacing: -2px; padding:10px 0 0 0; } #header a.blankheader { display:block; height: 64px; width:300px; } #header a { text-decoration:none; color: #27a5c4; border-bottom:none; } #header .description { font-family:Georgia, serif; font-size:18px; font-weight:normal; font-style:italic; margin-top: -0.25em; padding-bottom: 5px; }  #header a.subscribe-button  {  display:block; float:right; margin-top:-20px; } #header a.subscribe-button:hover {  margin-top:-10px; } /* NAVIGATION */ /* =Menu -------------------------------------------------------------- */  #access { /*background: #000;*/ margin: 0 auto; width: 100%; display:block; float:left; /*padding: 1px 0 1px 8px;*/ border-top: 4px solid #27a5c4; border-bottom: 2px solid #27a5c4; background-color: #b7dee8; } #access .menu-header, div.menu { font-size: 13px; margin-left: 12px; } #access .menu-header ul, div.menu ul { list-style: none; margin: 0; } #access .menu-header li, div.menu li { float:left; position: relative; margin: 0; } #access a { display:block; text-decoration:none; padding:0 10px; line-height:38px; color: #707070;  font-family:Trebuchet MS, Helvetica, sans-serif; font-size:18px; font-weight:normal;   } #access ul ul { display:none; position:absolute; top:38px; left:0; float:left; z-index: 99999; border-left:  1px solid #27a5c4; border-right: 1px solid #27a5c4; border-bottom:1px solid #27a5c4; background-color: #b7dee8; } #access ul ul ul { left:100%; top:1px; border-top:1px solid #27a5c4;  } #access ul ul a { font-family:Trebuchet MS, Helvetica, sans-serif; font-size:13px; font-weight:normal; } #access ul ul a { /*background:#333;*/ height:auto; line-height:1em; padding:10px; width: 170px; } #access li:hover > a, #access ul ul :hover > a { color:#000000; /*background:#333;*/  } #access ul li:hover > ul { display:block; }  /* menu-item menu-item-type-post_type current-menu-ancestor current-menu-parent current_page_parent current_page_ancestor menu-item menu-item-type-post_type current-menu-item page_item page-item-155 current_page_item */  #access .current-menu-item, #access .current_page, #access .current_page_item, #access .current_page_item a, #access .current-menu-ancestor, #access .current_page_ancestor { color: #505050; background-color: #8bd6e0; }  div.post-list-column { float:left; width:50%; } div.post-list-column .margins { padding: 0 10px 15px 10px; } .post-list-column .storycontent, .post-list-column .content { padding:0 0 0 4px; } .post-thumbnail, .regular-post-thumbnail { border: 1px solid #eeeeee; padding:3px; }  .regular-post-thumbnail { float:left; width: 100px; margin:18px 5px 10px 10px; } .wp-post-image { max-width:100%; border: none; }  div.post-list-column .wp-post-image { display:block; }  #postpath, #postpath a { font-family:Trebuchet MS, Helvetica, sans-serif; font-size:13px; font-weight:normal; color:#707070; }  /* LINKS */  a { color:#a17244; text-decoration:underline; } .post a:visited { color: #756455; text-decoration:underline; } .post a:hover { color: #de8435; text-decoration:underline; } .post h1 a:hover,  .post h2 a:hover { border-bottom:none; text-decoration:none; }  /* HEADINGS */  h1,h2,h3, h4, h5, h6 { color: #505050; margin-top:5px; font-family:Georgia, serif; font-weight: normal; } h1, h2.h1 { font-size:36px; font-weight: normal; margin:0px 0 3px 0; padding:10px 0 0 5px; color: #505050; overflow:hidden; } .post-list-column h2 { font-size: 24px; font-weight: normal; margin:0px 0 0px 0; padding:3px 0 0px 4px; color: #505050; } h2 { /*font-size:24px;*/ margin:5px 0 2px 0; color: #505050; font-size:24px; font-weight: normal; font-family:Georgia, serif; } h3 { margin:5px 0 0px 0; font-size:18px; font-weight: normal; font-family:Georgia, serif; } .post h3 { color: #98293D; } h4 { font-size:13px; font-weight: bold; font-family:Verdana, Geneva, sans-serif; } h1 a, h2.h1 a, h1 a:visited, h2.h1 a:visited, .post h1 a, .post h2.h1 a, .post h1 a:visited, .post h2.h1 a:visited, .post-list-column h2 a, .post-list-column h2 a:visited { color: #505050; border-bottom: none; text-decoration:none; } h1 a:hover, h2.h1 a:hover, h1 a:visited:hover, h2.h1 a:visited:hover, .post h1 a:hover, .post h2.h1 a:hover, .post h1 a:visited:hover, .post h2.h1 a:visited:hover, .post-list-column h2 a:hover, .post-list-column h2 a:visited:hover { color: #000000; border-bottom: none; text-decoration:none; } h1.page-title { font-size:18px; } h1.page-title span { color: #27a5c4; } /* POST */ div.post { overflow:hidden; } table.info { padding:0; margin:0; border-collapse:collapse; } table.info td,  table.info th { padding:0; margin:0; font-weight:normal; } pre { border:1px dotted #cccccc;; margin:5px 0; padding:10px 10px 10px 20px; background-color:#fbfbfb; } blockquote{ /*background:#f2f2f2 url(img/blockquote.gif) 3px 3px no-repeat;*/ border:1px dotted #cccccc; padding:10px 10px 10px 20px; margin:5px 0 5px 20px; background-color:#fbfbfb; } code { background-color:#fbfbfb; font-family: Courier New, Courier, monospace; border:1px dotted #cccccc; } .post .storycontent, .post .content { padding:5px 10px 5px 10px; overflow:hidden; line-height: 1.5; }  .post .info { padding:3px 0px 3px 0px; margin: 2px 0 2px 0; border-top: 1px solid #eeeeee; }  .post .date { background-position:0 -48px; color: #707070; padding:0 0px; } .post .info  .postedby, .post .info  .filledunder { color: #bbb; } .post .info  .postedby  a, .post .info  .filledunder a { color: #707070; text-decoration:none; } .post .info  .postedby a:hover, .post .info  .filledunder a:hover { color: #000000; text-decoration:none; } .post .info td { border: none; padding: 0 5px; } .post table.info { width:100%; } .post .info .act { white-space: nowrap; text-align:right; } .post table.info .date { width:1%; white-space: nowrap; } .post .act span { padding-left:15px; } .post .info * { font-family: Verdana, Geneva, sans-serif; line-height:16px; font-size:10px; overflow:hidden; } hr { border: 0px none; background-color: #eeeeee; color: #eeeeee; height:2px; } fieldset{ border: 1px solid #eeeeee; padding:5px 10px 5px 10px; } legend { padding: 0px 5px 1px 5px; border: 1px solid #eeeeee; } input.text, input.textbox, input.password, input.file, textarea, input[type=text], input[type=password],input[type=file], select { border:1px solid #bbbbbb; background-color:#ffffff; padding:2px; color: #505050;  } input.text:focus, input.textbox:focus, input.password:focus, input.file:focus, textarea:focus, input[type=text]:focus, input[type=password]:focus,input[type=file]:focus, select { border:1px solid #505050; }  .post table { border-collapse:collapse; border: none; } .post  th, .post  td { border-bottom:1px solid #eeeeee; border-left:none; border-right:none; padding:2px 10px; text-align:left; vertical-align:top;  font-size: 90%;  font-weight: normal; font-family: Trebuchet MS, Helvetica, sans-serif;  } .post tr.even td { background-color: #b7dee8; } .post th  { background-color: #b7dee8; border-bottom: 1px solid #27a5c4; border-top: 2px solid #27a5c4; color: #707070; } #author-avatar { float:left; width: 100px; margin:7px 15px 7px 10px; }  /* comment START */ /* COMMENTS */ #comments > ol {list-style-type: none;line-height: 18px;margin: 0px;padding: 0px 0px 10px 10px; text-align: justify;} #comments ul li {list-style-type: none;list-style-image: none;list-style-position: outside;margin: 0 0 0 5px; padding: 5px 0 0 0;} .commentlist li {margin: 15px 0 10px;padding: 2px 2px 5px 2px;list-style: none;} .commentlist li > ul > li {background:none;list-style:none;margin:3px 0 3px 20px;padding:3px 0;} .commentlist li .avatar {border:none; margin:0;padding:1px 8px 1px 1px; width: 26px;float:left; background:none;} .commentlist .fn { font-size: 14px;  font-weight: bold; font-style: normal; padding:4px 2px 2px 2px;  font-family: Verdana, Geneva, sans-serif; } .commentlist .fn > a  { font-weight:bold; font-style: normal; text-decoration:none; } .commentlist .fn a:hover {/*text-decoration:underline;*/} .commentmetadata  { /*color:#723419;*/ font-weight: normal;  font-family: Trebuchet MS, Helvetica, sans-serif; font-size:11px;  margin:0 0 0px 20px; text-decoration: none; }  .commentmetadata a  { font-weight: normal; text-decoration: none; color: #707070; }   .vcard a.url{ color: #707070; /*#723419;*/ text-decoration: none; } .vcard a.url:hover{ color: #000000; /*#723419;*/ text-decoration: none; }  .bypostauthor >.vcard div.fn >a { color:#98293D; } .bypostauthor >.vcard div.fn >a:hover { color:#000000; } .bypostauthor>div { color:#98293D; } /*  Uncomment following text to assign specific color to admin (or to any other user) You may need to change user name here E.g.  comment-author-MyUserName instead of  comment-author-admin */ /* .comment-author-admin>*, .comment-author-admin >.vcard div.fn >a { color:#106000; }*/ .comment { color:#505050; } .commentmetadata a, .commentmetadata a:visited {color: #707070;} .commentmetadata a:hover{ color: #000000;} #comments .children { padding: 0 0 0 20px; } .thread-alt {background-color:transparent} .thread-even {background-color: transparent;} .depth-1  { border: 1px solid #27a5c4; } .depth-2, .depth-3{/*border-top: 1px solid #dac2a3;*/} .even, .alt {} .vcard { background-color:#b7dee8;} .depth-2 .vcard, .depth-3 .vcard, .depth-4 .vcard, .depth-5 .vcard, .depth-6 .vcard, .depth-7 .vcard, .depth-8 .vcard { border-top: 1px dotted #27a5c4; border-bottom: 1px dotted #27a5c4; } .reply {margin: 0px 0px 0px 10px;} .comment-reply-link { background-color:#b7dee8;color: #707070; padding: 1px 4px; font-size:12px; text-decoration: none; border: 1px dotted #27a5c4; } .comment-reply-link:hover { color: #000000; text-decoration: none; }   #comments .comment-body ul li { list-style:square; margin: 0 0 0 30px; padding:0; } #comments .comment-body ol  { margin: 0; padding: 0; } #comments .comment-body ol li { list-style-type:decimal; padding:0; margin: 0 0 0 30px; display: list-item; } .comment-body { padding:2px 2px 2px 10px; }  /************************** IMAGES *************************************/  .post img.wp-caption, .wp-caption, .gallery-caption { -moz-border-radius: 3px; /* Firefox */ -webkit-border-radius: 3px; /* Safari, Chrome */ -khtml-border-radius: 3px; /* KHTML */ border-radius: 3px; /* CSS3 */    border:1px solid #eeeeee; display:block; height:auto; margin-bottom:10px; padding-top:4px; text-align:center; max-width:100%; } .regular-post-thumbnail img.attachment-post-thumbnail { display:block; margin: auto; } .post img.wp-caption{ padding:4px; } .post .wp-caption img, .wp-caption img { border:0 none !important; margin:0 !important; padding:0 !important; max-width:99.5%; } .post img { border:none; padding:0px; vertical-align:bottom; height:auto; max-width:100%; } a.wp-caption { color: #505050; text-decoration:none; } a.wp-caption p,a.wp-caption:hover p, .wp-caption p.wp-caption-text, .full-image-caption { color:#505050; font-size:11px; font-family:Verdana, Geneva, sans-serif; line-height:12px; margin:0; padding:2px 4px 4px; } a.wp-caption:hover { border:1px solid /*#dac2a3;*/#777777; background-color:#f2f2f2; }   /****************************************************************************************** SIDEBAR *******************************************************************************************/ #sidebar .subscribe-rss { padding:10px 0 0px 42px; height:30px; display:block; font-size:20px; font-family:Verdana,Geneva,sans-serif; margin:0 0 10px 0; }  #sidebar a.subscribe-rss:hover { color:#000000; text-decoration:none; } .sidebar { font-family:Georgia, serif; font-size:13px; }  .widget { margin-bottom:10px; line-height:1.5; overflow:hidden; } .textwidget { padding:10px 5px 10px 5px; border-bottom: 1px dotted #eeeeee; line-height:1.5; } .sidebar h5 { font-size:22px; font-family: Georgia, serif; font-weight:normal; border-bottom: 2px solid #eeeeee; padding: 5px 5px 10px 10px; } .sidebar a { color: #73b655;/*#64ae42;*/ font-size:13px; text-decoration:none; } .sidebar .widget_text a, .sidebar .widget_text a:hover { text-decoration:underline; } .sidebar a:hover { color: #000000; text-decoration:none; } .sidebar li { list-style:none; margin:0; padding: 7px 5px; } .sidebar .widget>ul>li, .sidebar ul.menu>li { border-bottom: 1px dotted #eeeeee; } .sidebar .widget>ul>li>.children, .sidebar .widget .sub-menu { padding-top:7px; } .sidebar li>ul li { padding: 7px 5px 7px 10px; } .sidebar li a { /*display: block;*/ } .sidebar .blogroll li { color:  #a0a0a0; font-size: 11px; } #wp-calendar table { border-collapse:collapse; border:0px solid #eeeeee; } #wp-calendar th,  #wp-calendar  td  { border:none; padding:0px 4px; vertical-align:top; font-family: Georgia, serif; font-size:90%; } #wp-calendar td { text-align:right; } #wp-calendar th  { background-color: #eeeeee; color:#ffffff; text-align:center; padding: 1px 4px 1px 4px; } #wp-calendar caption { font-family: Georgia, serif; font-weight:bold; /*color:#777777;*/ padding:10px 0 2px 0; }  #wp-calendar td#prev { text-align:left; } #wp-calendar td#next { text-align:right; } #wp-calendar #today  /*TODO!*/ { border: 1px dotted #dddddd; background-color: #b7dee8; }  /*Search widget*/  #searchform label  { display:block; } #s { width:110px; } #searchsubmit, .button, input#submit { border:1px solid #505050; color:#ffffff; font-weight:bold; font-family:Trebucht MS, Arial; text-shadow:0 -1px 0 rgba(0, 0, 0, 0.3); cursor: pointer; padding:1px 5px 1px 5px; background: #555555;  /* for non-css3 browsers */ filter: progid:DXImageTransform.Microsoft.gradient(startColorstr='#bbbbbb', endColorstr='#000000'); /* for IE */ background: -webkit-gradient(linear, left top, left bottom, from(#bbbbbb), to(#000000)); /* for webkit browsers */ background: -moz-linear-gradient(top,  #bbbbbb,  #000000); /* for firefox 3.6+ */ } #searchsubmit:hover, .button:hover { border:1px solid #000000; }  #searchsubmit:active, .button:active { cursor: pointer; filter: progid:DXImageTransform.Microsoft.gradient(startColorstr='#505050', endColorstr='#777777'); /* for IE */ background: -webkit-gradient(linear, left top, left bottom, from(#505050), to(#777777)); /* for webkit browsers */ background: -moz-linear-gradient(top,  #505050,  #777777); /* for firefox 3.6+ */ } /* PAGINATION */  #comments a.page-numbers, #comments span.page-numbers { text-decoration: none; border: 1px solid #eeeeee; padding: 2px 5px; margin: 2px; color: #505050; }  #comments .navigation { margin: 10px 0; }  #comments .navigation a:hover { border: 1px solid #505050; background-color: #f7f7f7; color: #505050; text-decoration:none; } #comments span.page-numbers { font-weight: bold; color: #ffffff; background-color: #505050; border: 1px solid #505050; }  /* WP-PageNavi http://wordpress.org/extend/plugins/wp-pagenavi/ */  .wp-pagenavi { clear: both; }  .wp-pagenavi a, .wp-pagenavi span  { text-decoration: none; border: 1px solid #eeeeee; padding: 2px 5px; margin: 2px; color: #505050; }  .wp-pagenavi a:hover { border: 1px solid #505050; background-color: #f7f7f7; color: #505050; text-decoration:none; }  .wp-pagenavi span.current { font-weight: bold; color: #ffffff; background-color: #505050; border: 1px solid #505050; } .wp-pagenavi .extend { background:transparent;  border:0px none transparent;  color: #505050; margin-right:6px;  padding:0;  text-align:center;  text-decoration:none; }  /* used by PAGBEAR plugin for multipaged posts. http://wordpress.org/extend/plugins/pagebar/ */ .pagebar { padding:0; margin:4px 0; }  .pagebar a { background:transparent;  border: 1px solid #eeeeee; color: #505050; margin: 2px; padding:2px 5px; text-align:center; text-decoration:none; } .pageList .this-page  { font-weight: bold; color: #ffffff; background-color: #505050; border: 1px solid #505050; margin: 2px; padding:2px 5px; text-align:center; text-decoration:none; } .pagebar a:visited { color: #505050; text-decoration:none; }  .pagebar .break { background:transparent;  border:0px none transparent;  color: #505050; margin-right:6px;  padding:0;  text-align:center;  text-decoration:none; }  .pagebar .this-page { font-weight: bold; color: #ffffff; background-color: #505050; border: 1px solid #505050; margin: 2px; padding:2px 5px; text-align:center; text-decoration:none; }  .pagebar a:hover { border: 1px solid #505050; background-color: #f7f7f7; color: #505050; text-decoration:none; }  .pagebar .inactive { border: 1px solid #eeeeee; background-color: #ffffff; color: #cccccc; text-decoration: none; padding:2px 4px; }  #postnavi .prev a  { float:left; } #postnavi .next a { float:right; } #postnavi  a  { background:transparent;  border: 1px solid #eeeeee; color: #505050; margin: 2px; padding:2px 5px; text-align:center; text-decoration:none; } #postnavi a:hover { border: 1px solid #505050; background-color: #f7f7f7; color: #505050; text-decoration:none; } /* FOOTER */   .footer-column  { margin:0; padding:10px; line-height:1.5; }  #blue-footer { float:left; width: 100%; } #green-footer { float:left; width: 100%; } #orange-footer { float:left; width: 100%; } #red-footer { float:left; width: 100%; } #footer h5 { font-size:22px; font-family: Georgia, serif; font-weight:normal; border-bottom: 2px solid #eeeeee; padding: 5px 5px 10px 10px; } #footer  a { color: #73b655;/*#64ae42;*/ font-size:13px; text-decoration:none; } #footer  .widget_text a, #footer .widget_text a:hover { text-decoration:underline; } #footer  a:hover { color: #000000; text-decoration:none; } #footer  li { list-style:none; margin:0; padding: 7px 5px; } #footer .widget>ul>li, #footer  ul.menu>li { border-bottom: 1px dotted #eeeeee; } #footer  .widget>ul>li>.children, #footer  .widget .sub-menu { padding-top:7px; } #footer  li>ul li { padding: 7px 5px 7px 10px; } /*#footer li a { display: block; }*/   </style>	<link rel="stylesheet" href="../wp-content/themes/clear-line/style.css" type="text/css" media="screen" />
	
		<link rel='archives' title='August 2011' href='index.html' />
	<link rel='archives' title='July 2011' href='../07/index.html' />
	<link rel='archives' title='June 2011' href='../06/index.html' />
	<link rel='archives' title='May 2011' href='../05/index.html' />
	<link rel='archives' title='April 2011' href='../04/index.html' />
	<link rel='archives' title='March 2011' href='../03/index.html' />
		                        <style type="text/css" media="screen" >
                                .socialwrap li.icon_text a img, .socialwrap li.iconOnly a img, .followwrap li.icon_text a img, .followwrap li.iconOnly a img{border-width:0 !important;background-color:none;}#follow.top {width:100%; position:fixed; left:0px; top:0px;}#follow.top ul {padding-left:20px;list-style-type:none;}#follow.top ul li {float:left;padding-top:4px;margin-right:10px;list-style-type:none;}#follow.top ul li.follow {color:#000;line-height:24px;}body {padding-top:32px}#follow a {text-decoration:none}
li.icon_text a span.head{}
li.icon_text a:hover span {border-bottom:solid 1px blue;}.share {margin:0 3px 3px 0;}
.phat span {display:inline;}
ul.row li {float:left;list-style-type:none;}
li.iconOnly a span.head {display:none}
#follow.left ul.size16 li.follow{margin:0px auto !important}
li.icon_text a {padding-left:0;margin-right:3px}
li.text_only a {background-image:none !important;padding-left:0;}
li.text_only a img {display:none;}
li.icon_text a span{background-image:none !important;padding-left:0 !important; }
li.iconOnly a span.head {display:none}
ul.socialwrap li {margin:0 3px 3px 0 !important;}
ul.socialwrap li a {text-decoration:none;}ul.row li {float:left;line-height:auto !important;}
ul.row li a img {padding:0}.size16 li a,.size24 li a,.size32 li a, .size48 li a, .size60 li a {display:block}ul.socialwrap {list-style-type:none !important;margin:0; padding:0;text-indent:0 !important;}
ul.socialwrap li {list-style-type:none !important;background-image:none;padding:0;list-style-image:none !important;}
ul.followwrap {list-style-type:none !important;margin:0; padding:0}
ul.followwrap li {margin-right:3px;margin-bottom:3px;list-style-type:none !important;}
#follow.right ul.followwrap li, #follow.left ul.followwrap li {margin-right:0px;margin-bottom:0px;}
.shareinpost {clear:both;padding-top:0px}.shareinpost ul.socialwrap {list-style-type:none !important;margin:0 !important; padding:0 !important}
.shareinpost ul.socialwrap li {padding-left:0 !important;background-image:none !important;margin-left:0 !important;list-style-type:none !important;text-indent:0 !important}
.socialwrap li.icon_text a img, .socialwrap li.iconOnly a img{border-width:0}ul.followrap li {list-style-type:none;list-style-image:none !important;}
div.clean {clear:left;}
div.display_none {display:none;}
                        </style>
                                                <style type="text/css" media="print" >
                                body {background: white;font-size: 12pt;color:black;}
 * {background-image:none;}
 #wrapper, #content {width: auto;margin: 0 5%;padding: 0;border: 0;float: none !important;color: black;background: transparent none;}
 a { text-decoration : underline; color : #0000ff; }
#menu, #navigation, #navi, .menu {display:none}
                        </style>
                        <link rel="image_src" href="#" /> 
<link rel='dns-prefetch' href='http://s.w.org/' />
<link rel="alternate" type="application/rss+xml" title="LUDEP &raquo; Feed" href="../../feed/index.html" />
<link rel="alternate" type="application/rss+xml" title="LUDEP &raquo; Comments Feed" href="../../comments/feed/index.html" />
<!-- This site is powered by Shareaholic - https://shareaholic.com -->
<script type='text/javascript' data-cfasync='false'>
  //<![CDATA[
    _SHR_SETTINGS = {"endpoints":{"local_recs_url":"https:\/\/www.ludep.com\/wp-admin\/admin-ajax.php?action=shareaholic_permalink_related"}};
  //]]>
</script>
<script type='text/javascript' data-cfasync='false'
        src='../../../dsms0mj1bbhn4.cloudfront.net/assets/pub/shareaholic.js'
        data-shr-siteid='dfcda9e11023ab5708e53ad765e54924' async='async' >
</script>

<!-- Shareaholic Content Tags -->
<meta name='shareaholic:site_name' content='LUDEP' />
<meta name='shareaholic:language' content='en-US' />
<meta name='shareaholic:site_id' content='dfcda9e11023ab5708e53ad765e54924' />
<meta name='shareaholic:wp_version' content='7.6.1.2' />

<!-- Shareaholic Content Tags End -->
		<script type="text/javascript">
			window._wpemojiSettings = {"baseUrl":"https:\/\/s.w.org\/images\/core\/emoji\/11.2.0\/72x72\/","ext":".png","svgUrl":"https:\/\/s.w.org\/images\/core\/emoji\/11.2.0\/svg\/","svgExt":".svg","source":{"concatemoji":"https:\/\/www.ludep.com\/wp-includes\/js\/wp-emoji-release.min.js?ver=5.1.8"}};
			!function(a,b,c){function d(a,b){var c=String.fromCharCode;l.clearRect(0,0,k.width,k.height),l.fillText(c.apply(this,a),0,0);var d=k.toDataURL();l.clearRect(0,0,k.width,k.height),l.fillText(c.apply(this,b),0,0);var e=k.toDataURL();return d===e}function e(a){var b;if(!l||!l.fillText)return!1;switch(l.textBaseline="top",l.font="600 32px Arial",a){case"flag":return!(b=d([55356,56826,55356,56819],[55356,56826,8203,55356,56819]))&&(b=d([55356,57332,56128,56423,56128,56418,56128,56421,56128,56430,56128,56423,56128,56447],[55356,57332,8203,56128,56423,8203,56128,56418,8203,56128,56421,8203,56128,56430,8203,56128,56423,8203,56128,56447]),!b);case"emoji":return b=d([55358,56760,9792,65039],[55358,56760,8203,9792,65039]),!b}return!1}function f(a){var c=b.createElement("script");c.src=a,c.defer=c.type="text/javascript",b.getElementsByTagName("head")[0].appendChild(c)}var g,h,i,j,k=b.createElement("canvas"),l=k.getContext&&k.getContext("2d");for(j=Array("flag","emoji"),c.supports={everything:!0,everythingExceptFlag:!0},i=0;i<j.length;i++)c.supports[j[i]]=e(j[i]),c.supports.everything=c.supports.everything&&c.supports[j[i]],"flag"!==j[i]&&(c.supports.everythingExceptFlag=c.supports.everythingExceptFlag&&c.supports[j[i]]);c.supports.everythingExceptFlag=c.supports.everythingExceptFlag&&!c.supports.flag,c.DOMReady=!1,c.readyCallback=function(){c.DOMReady=!0},c.supports.everything||(h=function(){c.readyCallback()},b.addEventListener?(b.addEventListener("DOMContentLoaded",h,!1),a.addEventListener("load",h,!1)):(a.attachEvent("onload",h),b.attachEvent("onreadystatechange",function(){"complete"===b.readyState&&c.readyCallback()})),g=c.source||{},g.concatemoji?f(g.concatemoji):g.wpemoji&&g.twemoji&&(f(g.twemoji),f(g.wpemoji)))}(window,document,window._wpemojiSettings);
		</script>
		<style type="text/css">
img.wp-smiley,
img.emoji {
	display: inline !important;
	border: none !important;
	box-shadow: none !important;
	height: 1em !important;
	width: 1em !important;
	margin: 0 .07em !important;
	vertical-align: -0.1em !important;
	background: none !important;
	padding: 0 !important;
}
</style>
	<link rel='stylesheet' id='wp-block-library-css'  href='../../wp-includes/css/dist/block-library/style.min9738.css?ver=5.1.8' type='text/css' media='all' />
<link rel='stylesheet' id='wpt-twitter-feed-css'  href='../wp-content/plugins/wp-to-twitter/css/twitter-feed9738.css?ver=5.1.8' type='text/css' media='all' />
<!-- This site uses the Google Analytics by MonsterInsights plugin v5.5.4 - Universal enabled - https://www.monsterinsights.com/ -->
<script type="text/javascript">
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
		m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','../../../www.google-analytics.com/analytics.js','__gaTracker');

	__gaTracker('create', 'UA-22499108-1', 'auto');
	__gaTracker('set', 'forceSSL', true);
	__gaTracker('send','pageview');

</script>
<!-- / Google Analytics by MonsterInsights -->
<script type='text/javascript' src='../../wp-includes/js/jquery/jqueryb8ff.js?ver=1.12.4'></script>
<script type='text/javascript' src='../../wp-includes/js/jquery/jquery-migrate.min330a.js?ver=1.4.1'></script>
<script type='text/javascript' src='../../wp-includes/js/jquery/ui/core.mine899.js?ver=1.11.4'></script>
<script type='text/javascript'>
/* <![CDATA[ */
var social_connect_data = {"wordpress_enabled":""};
/* ]]> */
</script>
<script type='text/javascript' src='../wp-content/plugins/social-connect/media/js/connect9738.js?ver=5.1.8'></script>
<link rel='https://api.w.org/' href='../../wp-json/index.html' />
<link rel="EditURI" type="application/rsd+xml" title="RSD" href="../../xmlrpc0db0.php?rsd" />
<link rel="wlwmanifest" type="application/wlwmanifest+xml" href="../../wp-includes/wlwmanifest.xml" /> 

<link rel="shortcut icon" href="../wp-content/plugins/multicons/images/favicon.ico" />



<link rel='stylesheet' id='social_connect-css'  href='../wp-content/plugins/social-connect/media/css/style9738.css?ver=5.1.8' type='text/css' media='all' />
<link rel='stylesheet' id='dashicons-css'  href='../../wp-includes/css/dashicons.min9738.css?ver=5.1.8' type='text/css' media='all' />
<link rel='stylesheet' id='wp-jquery-ui-dialog-css'  href='../../wp-includes/css/jquery-ui-dialog.min9738.css?ver=5.1.8' type='text/css' media='all' />
		<style type="text/css">
						ol.footnotes li {list-style-type:decimal;}
						ol.footnotes{font-size:0.8em; color:#666666;}		</style>
		<link rel="Shortcut Icon" type="image/x-icon" href="../../favicon.ico" />
</head>
<body class="archive date">
	<div id="container" class="hfeed">
	<div id="header">
		<div id="masthead">
						<div style="float:left;">
																						<h3><a href="../../index.html">LUDEP</a></h3>
													<div class="description">Leading Units &amp; Drone Enabled Probing</div>
												</div>
			<div class="clear"></div>
		</div>
					<div id="access">
				<div class="menu"><ul>
<li class="page_item page-item-6"><a href="../../index.html">Home</a></li>
<li class="page_item page-item-9 current_page_parent"><a href="../../blog/index.html">Blog</a></li>
<li class="page_item page-item-47"><a href="../../project/index.html">The project</a></li>
<li class="page_item page-item-50"><a href="../../links-downloads/index.html">Links &#038; Downloads</a></li>
<li class="page_item page-item-30"><a href="../../about/index.html">About</a></li>
</ul></div>
				<div class="clear"></div>
			</div>
				<div class="clear"></div>
	</div>
		<div class="sidebar">
					</div>


<!-- end header --><div id="wrapper">
	<div id="content-wrapper">
		<div id="content" role="main">
			
	<h1 class="page-title">
								Monthly Archives: <span>August 2011</span>			</h1>
  
  
				<div id="post-935" class="post post-935 type-post status-publish format-standard has-post-thumbnail hentry category-ground-robots category-ideas category-issues category-programming category-refining-the-project category-testing">
		<h2 class="h1"><a href="../../the-flock-behavior-from-scratch-till-now/index.html" rel="bookmark">The flock behavior: from scratch till now</a></h2>

					<table class="info entry-meta">
				<tr>
					<td class="date">August 23, 2011</td>
					<td>
													<span class="postedby">
								Posted by <a href="../../index.html" title="Visit Guillaume&#8217;s website" rel="author external">Guillaume</a> 
							</span>
								<span class="filledunder">under</span>
												<span class="filledunder">
							<a href="../../category/ground-robots/index.html" rel="category tag">Ground robots</a>, <a href="../../category/ideas/index.html" rel="category tag">Ideas</a>, <a href="../../category/issues/index.html" rel="category tag">Issues</a>, <a href="../../category/programming/index.html" rel="category tag">Programming</a>, <a href="../../category/refining-the-project/index.html" rel="category tag">Refining the project</a>, <a href="../../category/testing/index.html" rel="category tag">Testing</a>						</span>
					</td>
											<td>
						
				
							<div class="act">
																	<span class="comments">Comments off</span>
																							</div>
						</td>
					
				</tr>
			</table>
				
									
		
		
		<div class="content">
			
			<div style="float:right;padding: 0 0 10px 10px" class="interactive_right"><iframe allowtransparency="true" frameborder="0" scrolling="no" src="http://platform.twitter.com/widgets/tweet_button.html?url=https%3A%2F%2Fwww.ludep.com%2Fthe-flock-behavior-from-scratch-till-now%2F&amp;text=The%20flock%20behavior:%20from%20scratch%20till%20now&amp;count=vertical&amp;lang=" style="width:65px; height:65px;"></iframe><iframe src="http://www.facebook.com/plugins/like.php?href=https%3A%2F%2Fwww.ludep.com%2Fthe-flock-behavior-from-scratch-till-now%2F&amp;layout=box_count&amp;show_faces=false&amp;width=65&amp;action=like&amp;font=arial&amp;colorscheme=light&amp;height=65" scrolling="no" frameborder="0" style="border:none; overflow:hidden; width:65px; height:65px;" allowTransparency="true"></iframe></div><h2 style="text-align: justify;">Introduction</h2>
<p style="text-align: justify;">The  flock behavior is one of the last things we had to deal with because of all the amount of work before that, but it is one of the most important feature of the project and we spent a lot of time implementing, coding and testing it in order to make it close to our expectations. This article will be the most important concerning the flock behavior: it might talk about points that have already been mentioned before but that is only in order to give more details and further explanations.</p>
<p style="text-align: justify;">Our supervisor let us know that a former student already worked on mobile robots moving in formation<sup><a href="#footnote_0_935" id="identifier_0_935" class="footnote-link footnote-identifier-link" title="PhD dissertation by Jakob Fredslund; Simplicity Applied in Projects Involving Embodied, Autonomous Robots; pp67-124">1</a></sup>. It was really interesting to see how a same project can be approached in different ways: instead of giving most of the control to the robots in the flock, we decided to put the leader in charge for almost everything. This decision made the programming and implementation different but we came out with a very autonomous system at the end of the project.</p>
<p style="text-align: justify;">This article will be structured in the same way the flock behavior was designed: with progressive layers. We integrated one behavior at a time, tested it over and over and passed to the next one. In every point here, we will ask what we wanted to do, why we wanted to do it, what was expected and what were the results.</p>
<p>&nbsp;</p>
<h2 style="text-align: justify;">The different steps of the flock</h2>
<h3 style="text-align: justify;">The basic flock</h3>
<h4 style="text-align: justify;">Expectations</h4>
<p style="text-align: justify;">This was the really first approach we had. Basically, the image analysis would return two coordinates: one for the leader and the other for the robot following. The robot following the leader was only supposed to go &#8220;down&#8221; the leader (we take the coordinates of the leader and subtract a value called &#8220;desiredSpace&#8221; that will define the space between each neighbor unit in the formation). With such an implementation, we were expecting the robot following to act roughly like the leader with a little delay that we were ready to tackle if it was too important.</p>
<h4 style="text-align: justify;">Results</h4>
<p style="text-align: justify;">We posted videos in the <a href="../../merging-our-work-together-the-beginnings-of-worthy-and-notable-results/index.html" target="_blank">previous article</a> (under &#8220;Very first working test&#8221;) and we could see that the behavior was working but wasn&#8217;t fluid at all. The robot was properly changing its position according to the leader movements, we just needed to tune our parameters in order to make it more reactive, more suitable.</p>
<h4 style="text-align: justify;">Improvements</h4>
<p style="text-align: justify;">In order to understand how we improved the fluidity, we have to tell you more about how the flock is being monitored. The code in itself is really long, and it might be too much to expose it straight here; instead, a basic representation should do the trick and expose you the inner mechanics of the algorithm.</p>
<table class="aligncenter" width="485">
<tbody>
<tr>
<td><a href="../wp-content/uploads/2011/08/cycleFlock.png"><img class="aligncenter size-full wp-image-946" title="cycleFlock" src="../wp-content/uploads/2011/08/cycleFlock.png" alt="" width="485" height="335" srcset="../wp-content/uploads/2011/08/cycleFlock.png 809w, ../wp-content/uploads/2011/08/cycleFlock-300x207.png 300w, ../wp-content/uploads/2011/08/cycleFlock-100x69.png 100w" sizes="(max-width: 485px) 100vw, 485px" /></a><a href="../wp-content/uploads/2011/08/cycleFlock.png"><br />
</a></td>
</tr>
<tr>
<th style="text-align: justify;"><strong>[Figure 1]</strong> Cycle of the flock handling process</th>
</tr>
</tbody>
</table>
<p>&nbsp;</p>
<p style="text-align: justify;">As you might have pictured it in your mind, the big loop is indeed an infinite loop in our program. Every time one cycle has been done, the system makes a pause of a certain time (that we can settle) in order not to send too much commands to the bricks. Indeed, if hadn&#8217;t do that, the program would still work, but we would have a lot of useless information sent to the brick that wouldn&#8217;t even be treated because they would be erased by the next commands (if the robot is executing a command and receives a new one, it will abort the previous one and execute the most recent).</p>
<p style="text-align: justify;">All of those explanation to make you understand our system and to show you where we tuned our parameter in order to fix our delay problem. First, we change the pause in the loop from 1 second to 0,25 second (which made us gain in response time). Secondly, we change a layer in the &#8220;Assign and Send Movement Command&#8221;. Actually this minor change was operated on the brick in itself: the robot is asked to move with a speed inversely proportional to the distance it is asked to move. Nevertheless, an overshot problem was still remaining and we had to tune another parameter to fix it: every time a robot is on its desired location, we define a threshold, a circle of tolerance acknowledging the robot in good position or not. We changed it from 10cm to 30cm and made it work.</p>
<p style="text-align: justify;">At this point, tuning few parameters of our system and adding a proportional correction transformed our jerky system into a reliable and fluid one.</p>
<p>&nbsp;</p>
<h3 style="text-align: justify;">A dynamic and oriented flock</h3>
<h4 style="text-align: justify;">Expectations</h4>
<p style="text-align: justify;">At this point, one robot was properly following the other one. But the &#8220;Compute desired positions&#8221; layer was quite simple as we mentioned before: it only translated the position of the leader down and gave it to the other robot. We wanted to have a random number of robots in the flock and we wanted the flock to adapt itself in real-time according to this number. Besides, we wanted to add the &#8220;orientation&#8221; feature. Instead of staying below the leader, we wanted the flock to stay behind (that is to say in the opposite direction that the leader is looking at). Why such a choice you might say. Indeed, if our project was designed to work only in straight lines, we wouldn&#8217;t mind but we really wanted to implement something that would be coherent for direction changes. This feature would give the ability to the flock to follow the leader instead of simply staying behind, naively copying the leader&#8217;s gesture.</p>
<h4 style="text-align: justify;">How we implemented it</h4>
<p style="text-align: justify;">The algorithm is quite detailed once more; instead of explaining the code line by line or so, we split the functioning in its three biggest parts in order to show you the mechanics.</p>
<table>
<tbody>
<tr>
<td width="300"><a href="../wp-content/uploads/2011/08/computePositions1.png"><img class="aligncenter size-medium wp-image-958" title="computePositions1" src="../wp-content/uploads/2011/08/computePositions1-300x178.png" alt="" width="300" height="178" srcset="../wp-content/uploads/2011/08/computePositions1-300x178.png 300w, ../wp-content/uploads/2011/08/computePositions1-100x59.png 100w, ../wp-content/uploads/2011/08/computePositions1.png 784w" sizes="(max-width: 300px) 100vw, 300px" /></a></td>
<td width="300"><a href="../wp-content/uploads/2011/08/computePositions2.png"><img class="aligncenter size-medium wp-image-960" title="computePositions2" src="../wp-content/uploads/2011/08/computePositions2-300x178.png" alt="" width="300" height="178" srcset="../wp-content/uploads/2011/08/computePositions2-300x178.png 300w, ../wp-content/uploads/2011/08/computePositions2-100x59.png 100w, ../wp-content/uploads/2011/08/computePositions2.png 784w" sizes="(max-width: 300px) 100vw, 300px" /></a></td>
<td width="300"><a href="../wp-content/uploads/2011/08/computePositions3.png"><img class="aligncenter size-medium wp-image-961" title="computePositions3" src="../wp-content/uploads/2011/08/computePositions3-300x178.png" alt="" width="300" height="178" srcset="../wp-content/uploads/2011/08/computePositions3-300x178.png 300w, ../wp-content/uploads/2011/08/computePositions3-100x59.png 100w, ../wp-content/uploads/2011/08/computePositions3.png 784w" sizes="(max-width: 300px) 100vw, 300px" /></a></td>
</tr>
<tr>
<th style="text-align: justify;"><strong>[Figure 2]</strong> The algorithm detects the number of units detected (4 in our case)</th>
<th style="text-align: justify;"><strong>[Figure 3]</strong> It draws a polygon of n (number of units detected) sides with the leader being at the center&#8230;</th>
<th style="text-align: justify;"><strong>[Figure 4]</strong>&#8230;and then translates the polygon such as the first summit would be the leader. All the other summits represent the positions for the flock</th>
</tr>
</tbody>
</table>
<p>&nbsp;</p>
<p style="text-align: justify;">After the algorithm has run, we have a set of positions of the robots and a set of positions for the desired positions. The next challenge was to find the best position to go for each robot and make it optimal for the whole flock. For this matter, several solution were found:</p>
<ul>
<li>look at all the possible solutions and take the best one <strong>~ O(n!) </strong>which is barely acceptable even if we are working with four units at most (we rejected this solution because our algorithm is supposed to work with <em>n</em> robots and who could accept an algorithm with such a complexity&#8230;)</li>
<li>look at the best solution for each individual <strong>~ O(n²)</strong></li>
</ul>
<div>The second solution tends to be really close to the first (and best in term of distance but not in time) one, especially when there are not that many robots. This is why we chose that solution and instead of taking the units in the same order at each iteration, we randomize it every time (this would avoid some blocking problems).</div>
<h4 style="text-align: justify;">Results</h4>
<table class="aligncenter" width="420">
<tbody>
<tr>
<td><object width="420" height="345" classid="clsid:d27cdb6e-ae6d-11cf-96b8-444553540000" codebase="http://download.macromedia.com/pub/shockwave/cabs/flash/swflash.cab#version=6,0,40,0"><param name="allowFullScreen" value="true" /><param name="allowscriptaccess" value="always" /><param name="src" value="http://www.youtube.com/v/V3DXk8OqgwM?version=3&amp;hl=en_US" /><param name="allowfullscreen" value="true" /><embed width="420" height="345" type="application/x-shockwave-flash" src="http://www.youtube.com/v/V3DXk8OqgwM?version=3&amp;hl=en_US" allowFullScreen="true" allowscriptaccess="always" allowfullscreen="true" /></object></td>
</tr>
<tr>
<th style="text-align: justify;"><strong>[Video 1]</strong> Looking at the flock self-adjusting in real-time</th>
</tr>
</tbody>
</table>
<p>&nbsp;</p>
<p style="text-align: justify;">We had exactly what we wanted for this feature, we can exactly see how the different polygons shape with the robots coming on the field and this works whatever the direction of the leader. Nevertheless, the robots are not moving on this video because they could intersect each other&#8217;s path and therefore ruin the flock. This is why the next step was crucial: we had to deal with the avoidance. [Video 1]</p>
<h4 style="text-align: justify;">Improvements</h4>
<p style="text-align: justify;">The first version of the algorithm that computed the position for the robots only needed a radius for the circle containing the polygon of the units. Problem being: the more units we had in the flock, the closer they would be. In order to fix that, the radius of this circle had to change according to the number of present units in the flock. Using basic trigonometry, we came out with a formula for this radius that make the distance between neighbor units constant.</p>
<p><a href="../wp-content/uploads/2011/08/formulaCircle.png"><img class="aligncenter size-full wp-image-983" title="formulaCircle" src="../wp-content/uploads/2011/08/formulaCircle.png" alt="" width="377" height="71" srcset="../wp-content/uploads/2011/08/formulaCircle.png 377w, ../wp-content/uploads/2011/08/formulaCircle-300x56.png 300w, ../wp-content/uploads/2011/08/formulaCircle-100x18.png 100w" sizes="(max-width: 377px) 100vw, 377px" /></a></p>
<p style="text-align: justify;">This arrangement is indeed proper with a small number of robots but the bigger the flock gets, the bigger the polygon will be. This can be annoying, especially if we want to use the less space possible on the floor; we have an unused area proportional to the square of the number of units and the distance between them (2,8m² for 10 units for instance). In order to solve that, we could change the &#8220;Compute desired location&#8221; layer and instead of making one polygon of robots, we could make several polygons inside each other and save this way all the unused space. Here are some ideas we could think of on the Figure 5.</p>
<table class="aligncenter" width="571">
<tbody>
<tr>
<td><a href="../wp-content/uploads/2011/08/possibleFormations1.png"><img class="aligncenter size-full wp-image-1085" title="possibleFormations" src="../wp-content/uploads/2011/08/possibleFormations1.png" alt="" width="571" height="286" srcset="../wp-content/uploads/2011/08/possibleFormations1.png 815w, ../wp-content/uploads/2011/08/possibleFormations1-300x150.png 300w, ../wp-content/uploads/2011/08/possibleFormations1-100x50.png 100w" sizes="(max-width: 571px) 100vw, 571px" /></a></td>
</tr>
<tr>
<th style="text-align: justify;"><strong>[Figure 5]</strong> Template we could use for improved flock position (with numerous robots). On the left template, we multiply the units by two on the outer belt; on the right template, we add one more unit to the outer belt. Those are indeed examples, we don&#8217;t have a precise idea of what is or should be the best one, it is just fuel for thoughts.</th>
</tr>
</tbody>
</table>
<p>We could even think to dispatch the units all around the leader: the leader would be the center of all those nested polygons. It could be a real advantage if we have a lot of units in the flock: they would be all in an optimal layout in order to stay in the FOV. Nevertheless, we would have a lot of problem with the avoidance that we are going to develop in the next section.</p>
<h3 style="text-align: justify;">Implementing the movements of the flock</h3>
<h4 style="text-align: justify;">Expectations</h4>
<p style="text-align: justify;">At this point, we had everything working in theory. We mean in theory because every time a robot is asked to go to a position, it will go where it is ask to go without asking further question. Basically, this section describes how the &#8220;Assign and send movement commands to bricks&#8221; layer [Figure 1] is designed. In order to have a stable system, we had to answer/find a way to solve the following problems:</p>
<ul style="text-align: justify;">
<li>The robots in the flock should not bump into each other;</li>
<li>The robots should try to stay in the FOV of the camera in order to stay alive in the flock (if the robot is not detected for a while, it will be deleted from the flock);</li>
<li>The robots in the flock should do their best in order to avoid the leader when maneuvering;</li>
<li>Give a priority to all those behaviors in order to make everything coherent and working.</li>
</ul>
<h4 style="text-align: justify;">How we implemented it</h4>
<p style="text-align: justify;"><em><strong>How the robots avoid each other</strong></em></p>
<p style="text-align: justify;">As soon as the &#8220;Compute desired location&#8221; layer [Figure 1] has been executed, we have two things: a list of bricks in the flock with their positions and a set of desired positions (and they should be of the exact same length). Randomly, we are taking a robot and we are assigning it to a position. This is the point where the following algorithm is triggered:</p>
<pre>public void giveBrickDirection(Brick b, Point p)
{
	while( distancePositions(b.getPosition(), p) &gt; proximityThreshold )
	{
		if(movementPossible(b, p))
		{
			b.setGoToPostion(p);
			return;
		}
		else
		{
			p.translate((-p.x+b.getPosition().x)/3, (-p.y+b.getPosition().y)/3);
		}
	}
}</pre>
<p style="text-align: justify;">The mechanics are that simple: as long as the distance between the robot and its goal position is longer than the proximity threshold (this is the &#8220;error distance&#8221; we define in order to establish when a robot is close enough to its target), we look if the goal position is within range. If it is reachable, we send the command to the robot in order to send it to the desired position. If not, we reduce the distance of the command by 33%. The recursion is terminal because either the robot will be able to make the movement within few recursions or at one point the distance will get smaller than the threshold and the robot will stand still, waiting for the next command.</p>
<p style="text-align: justify;">The black box in here would be the method <em>movementPossible(Brick b, Point p)</em>. To make it understandable, the method goes through the list of all the brick and check if the movement of the considered brick <em>b</em> will not cross any other brick location or movement. In order to make it even simpler, take a look at the Figure 6.</p>
<table class="aligncenter" width="442">
<tbody>
<tr>
<td><a href="../wp-content/uploads/2011/08/noCollision1.png"><img class="aligncenter size-full wp-image-1086" title="noCollision" src="../wp-content/uploads/2011/08/noCollision1.png" alt="" width="442" height="400" srcset="../wp-content/uploads/2011/08/noCollision1.png 736w, ../wp-content/uploads/2011/08/noCollision1-300x271.png 300w, ../wp-content/uploads/2011/08/noCollision1-100x90.png 100w" sizes="(max-width: 442px) 100vw, 442px" /></a></td>
</tr>
<tr>
<th style="text-align: justify;"><strong>[Figure 6]</strong> How the &#8220;collision area&#8221; is defined. As long as nothing is into or enters the green area, the red robot will be allowed to make its move.</th>
</tr>
</tbody>
</table>
<p>&nbsp;</p>
<p style="text-align: justify;">If the red brick has to move, we will compare with every other brick (the blue one here for instance) that there is no conflict. If the blue brick is not moving (as in our example), we check if the initial position is not in the  &#8220;no collision area&#8221;; if not, the movement will be allowed. If the blue brick was moving, we would do two more checks: one to test if the blue robot&#8217;s desired location is not in the &#8220;no collision area&#8221; and we&#8217;ll check if the lines are intersecting with basic geometry rules.</p>
<p style="text-align: justify;"><em><strong>How the robots survive (from being excluded from the FOV)</strong></em></p>
<p style="text-align: justify;">This behavior was really simple to approach and implement, here is some code to illustrate it:</p>
<pre>if (  getConsideredBricks().get(i).isOnEdge() )
{
	giveBrickDirection(currentBrick,
			new Point((brickInControl.getPosition().x-brickPosition.x)/2+brickPosition.x,
					(brickInControl.getPosition().y-brickPosition.y)/2+brickPosition.y));
}</pre>
<p style="text-align: justify;">Basically, every brick has an boolean attribute &#8220;isOnEdge&#8221; given by the drone. If the robot is too close to the limit of the FOV, the attribute &#8220;isOnEdge&#8221; switches to <em>true</em> and we just ask the robot to get closer to the leader. With the drone being supposed to stay on top of the leader, the robot getting closer to the edge will therefore get closer to the leader/the center of the FOV.</p>
<p style="text-align: justify;"><em><strong>How the robots avoid the leader</strong></em></p>
<p style="text-align: justify;">This feature works exactly the same way than the previous one. We check if the robot is too close to the leader. If not, the robot might proceed to the normal routine (previously mentioned); if so, the robot has to &#8220;escape&#8221; from the leader. Why should we do complicated when a simple solution works a charm?</p>
<p style="text-align: justify;"><em><strong>How we merge all those behaviors together</strong></em></p>
<p style="text-align: justify;">How to order those behaviors would have an important impact on the global behavior of the system. If we take a closer, we can detect that some commands contradict each other (survival and leader avoidance for instance): this is why we took a long time in order prioritize each behavior according to the way we wanted the system to respond [Figure 7].</p>
<table class="aligncenter" width="390">
<tbody>
<tr>
<td><a href="../wp-content/uploads/2011/08/priorityBehaviors.png"><img class="aligncenter size-full wp-image-1076" title="priorityBehaviors" src="../wp-content/uploads/2011/08/priorityBehaviors.png" alt="" width="390" height="279" srcset="../wp-content/uploads/2011/08/priorityBehaviors.png 488w, ../wp-content/uploads/2011/08/priorityBehaviors-300x214.png 300w, ../wp-content/uploads/2011/08/priorityBehaviors-100x71.png 100w" sizes="(max-width: 390px) 100vw, 390px" /></a></td>
</tr>
<tr>
<th style="text-align: justify;"><strong>[Figure 7]</strong> The prioritization of the movement behavior</th>
</tr>
</tbody>
</table>
<p>&nbsp;</p>
<p style="text-align: justify;">Our first goal was to keep the most of the units in the flock, this is why the survival behavior (staying inside the FOV) behavior is the one with most priority. The leader avoidance is something crucial for us because we know how annoying it is to be blocked by another unit while trying to move in a world: this is why it has been given the second priority. And at last, if no one of the previous behavior has been activated, the normal movement behavior will be triggered. Indeed, in every of those case, when a robot is asked to go to a position, the <em>giveBrickDirection</em> method is called and we check if the robot is enabled to move or not (or we try to make a part of this movement).</p>
<h4 style="text-align: justify;">Results</h4>
<h4 style="text-align: justify;"><span class="Apple-style-span" style="font-weight: normal;">Well those videos are the one we took at the end of our project. Everything is working as we expected and described it above, using a webcam on the roof or using the drone. Nonetheless, if we had more time, there would be a lot more to do in order to improve this project and we will expose few ideas in the next section. In the meanwhile here is all the videos that are relevant to all that have been mentioned before. [Videos 2-5]</span></h4>
<p>&nbsp;</p>
<table class="aligncenter" width="840">
<tbody>
<tr>
<td><object width="420" height="345" classid="clsid:d27cdb6e-ae6d-11cf-96b8-444553540000" codebase="http://download.macromedia.com/pub/shockwave/cabs/flash/swflash.cab#version=6,0,40,0"><param name="allowFullScreen" value="true" /><param name="allowscriptaccess" value="always" /><param name="src" value="http://www.youtube.com/v/_rIb-HE_iLY?version=3&amp;hl=en_US" /><param name="allowfullscreen" value="true" /><embed width="420" height="345" type="application/x-shockwave-flash" src="http://www.youtube.com/v/_rIb-HE_iLY?version=3&amp;hl=en_US" allowFullScreen="true" allowscriptaccess="always" allowfullscreen="true" /></object></td>
<td><object width="420" height="345" classid="clsid:d27cdb6e-ae6d-11cf-96b8-444553540000" codebase="http://download.macromedia.com/pub/shockwave/cabs/flash/swflash.cab#version=6,0,40,0"><param name="allowFullScreen" value="true" /><param name="allowscriptaccess" value="always" /><param name="src" value="http://www.youtube.com/v/YDeeYbvr0e8?version=3&amp;hl=en_US" /><param name="allowfullscreen" value="true" /><embed width="420" height="345" type="application/x-shockwave-flash" src="http://www.youtube.com/v/YDeeYbvr0e8?version=3&amp;hl=en_US" allowFullScreen="true" allowscriptaccess="always" allowfullscreen="true" /></object></td>
</tr>
<tr>
<th style="text-align: justify;"><strong>[Video 2]</strong> <em>from above</em>: the green robot is the leader all the time it appears on the screen; as soon as it goes out of the FOV, the yellow one takes the lead. It is a great video if you want to see how the leader avoidance works: every time the leader changes direction and goes towards a unit, this very unit escapes the leader and lets the leader pass.</th>
<th style="text-align: justify;"><strong>[Video 3]</strong> <em>from above</em>: green has the lead. This time, we can see that the red goes out of the screen (the edges were defined very low for this video) and as soon it has disappeared, you can see the blue robot changing the formation.</th>
</tr>
<tr>
<td><object width="420" height="345" classid="clsid:d27cdb6e-ae6d-11cf-96b8-444553540000" codebase="http://download.macromedia.com/pub/shockwave/cabs/flash/swflash.cab#version=6,0,40,0"><param name="allowFullScreen" value="true" /><param name="allowscriptaccess" value="always" /><param name="src" value="http://www.youtube.com/v/CZzpG6ndnbg?version=3&amp;hl=en_US" /><param name="allowfullscreen" value="true" /><embed width="420" height="345" type="application/x-shockwave-flash" src="http://www.youtube.com/v/CZzpG6ndnbg?version=3&amp;hl=en_US" allowFullScreen="true" allowscriptaccess="always" allowfullscreen="true" /></object></td>
<td><object width="420" height="345" classid="clsid:d27cdb6e-ae6d-11cf-96b8-444553540000" codebase="http://download.macromedia.com/pub/shockwave/cabs/flash/swflash.cab#version=6,0,40,0"><param name="allowFullScreen" value="true" /><param name="allowscriptaccess" value="always" /><param name="src" value="http://www.youtube.com/v/2OZxAynznAM?version=3&amp;hl=en_US" /><param name="allowfullscreen" value="true" /><embed width="420" height="345" type="application/x-shockwave-flash" src="http://www.youtube.com/v/2OZxAynznAM?version=3&amp;hl=en_US" allowFullScreen="true" allowscriptaccess="always" allowfullscreen="true" /></object></td>
</tr>
<tr>
<th style="text-align: justify;"><strong>[Video 4]</strong> <em>from the field</em>: blue is the leader and we use the drone. You will see a lot of errors in this video: the yellow color is not well detected from time to time, the avoidance is not that well tuned (we had to increase the proximity threshold) and the orientation of the robot were not that well handled. The yellow unit died quite often even if we replaced it in the flock.</th>
<th style="text-align: justify;"><strong>[Video 5]</strong> <em>from the field</em>: blue is the leader and we use once more the drone. You can see the dynamic flock (at the beginning, when inserting unit one at the time), and all the avoidance behavior (all the units try not to bump into the leader and you can watch at the end, before the crash, how the red and the green unit slow down their speed in order not to cross each other&#8217;s path)</th>
</tr>
</tbody>
</table>
<p>&nbsp;</p>
<h4 style="text-align: justify;">Improvements</h4>
<p style="text-align: justify;">First of all, we needed the robot to keep a straight orientation. It could have been done with a compass sensor and we actually did it. But we couldn&#8217;t handle all the magnetic fields in the room we are working in and this has paralyzed our project in a certain way (the use of a PID correction in order to keep the orientation almost solved the problem). And this is the reason why you might see us on some videos putting the robots straight on the floor. So, one major improvement: restore the thread on the robots making them face the same direction as the drone.</p>
<p style="text-align: justify;">Second and biggest improvement: adding behaviors to the robots. They could indeed have an obstacle avoidance behavior in order to give them more responsibilities for instance. They could try to find their way back into the FOV when lost (and the rest of the flock could wait for it or even try to look for him). Giving the robots a more autonomous behavior would without a doubt improve the project, but we should always keep in mind that the leader is the hive-mind of the system and the robots have to give it the highest priority.</p>
<p>&nbsp;<br />
<b>References</b></p>
<ol class="footnotes">
<li id="footnote_0_935" class="footnote">PhD dissertation by <em>Jakob Fredslund; Simplicity Applied in Projects Involving Embodied, Autonomous Robots; pp67-124</em> [<a href="#identifier_0_935" class="footnote-link footnote-back-link">&#8617;</a>]</li>
</ol>
<div class='shareaholic-canvas' data-app-id='17416102' data-app='recommendations' data-title='The flock behavior: from scratch till now' data-link='https://www.ludep.com/the-flock-behavior-from-scratch-till-now/' data-summary=''></div>		</div>
		<div class="clear"></div>
		
	</div>
		<div id="post-1033" class="post post-1033 type-post status-publish format-standard has-post-thumbnail hentry category-drone category-ground-robots category-ideas category-programming category-testing">
		<h2 class="h1"><a href="../../final-words-on-the-drone-merging-previous-work-with-new-tasks/index.html" rel="bookmark">Final words on the drone: merging previous work with new tasks</a></h2>

					<table class="info entry-meta">
				<tr>
					<td class="date">August 23, 2011</td>
					<td>
													<span class="postedby">
								Posted by <a href="http://michael/" title="Visit Michael&#8217;s website" rel="author external">Michael</a> 
							</span>
								<span class="filledunder">under</span>
												<span class="filledunder">
							<a href="../../category/drone/index.html" rel="category tag">Drone</a>, <a href="../../category/ground-robots/index.html" rel="category tag">Ground robots</a>, <a href="../../category/ideas/index.html" rel="category tag">Ideas</a>, <a href="../../category/programming/index.html" rel="category tag">Programming</a>, <a href="../../category/testing/index.html" rel="category tag">Testing</a>						</span>
					</td>
											<td>
						
				
							<div class="act">
																	<span class="comments">Comments off</span>
																							</div>
						</td>
					
				</tr>
			</table>
				
									
		
		
		<div class="content">
			
			<div style="float:right;padding: 0 0 10px 10px" class="interactive_right"><iframe allowtransparency="true" frameborder="0" scrolling="no" src="http://platform.twitter.com/widgets/tweet_button.html?url=https%3A%2F%2Fwww.ludep.com%2Ffinal-words-on-the-drone-merging-previous-work-with-new-tasks%2F&amp;text=Final%20words%20on%20the%20drone:%20merging%20previous%20work%20with%20new%20tasks&amp;count=vertical&amp;lang=" style="width:65px; height:65px;"></iframe><iframe src="http://www.facebook.com/plugins/like.php?href=https%3A%2F%2Fwww.ludep.com%2Ffinal-words-on-the-drone-merging-previous-work-with-new-tasks%2F&amp;layout=box_count&amp;show_faces=false&amp;width=65&amp;action=like&amp;font=arial&amp;colorscheme=light&amp;height=65" scrolling="no" frameborder="0" style="border:none; overflow:hidden; width:65px; height:65px;" allowTransparency="true"></iframe></div><h2>Last automatic control improvements</h2>
<h3>What is wrong with our Proportional Integrative Derivative (PID) controller&#8230;</h3>
<p style="text-align: justify;">We have spent quite a good amount of time implementing, testing, tuning parameters and tweaking our PID controller over the last months. Our understanding of all its underlying theoretical aspects -mathematical, physical and computational- improved as much, and our results got generally better with time. Overall, we managed to get our drone to stay on top of a defined target, at a given altitude, without drifting much. However, our main concern was that, at some point, after a few minutes of running time, it would start describing circles around the target that would get bigger and bigger, or simply drift away. It was made clear that it was nothing due to our detection algorithm, but rather had to do with our PID.</p>
<p style="text-align: justify;">And indeed, when we started to take again some hindsight and got back on all our results and data, we confirmed our very first doubts. PID controllers are great when you have to stabilize a system with one degree of freedom; more precisely, they are efficient when you need to work with one error parameter, which can be corrected by a set of actuators that do not have influence of any sort on other error parameters which also have to be regulated. Here, with our drone, we face a situation with three degrees of freedom (i.e. pitch, roll, yaw) that we try to stabilize toward our goal, and thanks to the very same actuators (i.e. our four rotors). Well, so far, we have not even tried to mess with yawing, since it was not absolutely required to achieve our project. Therefore, every time our algorithm needs to correct one degree of freedom, it is influencing the correction of the other, which needs then a better correction, and it goes on toward more and more instability, making it an explosive system.</p>
<p style="text-align: justify;">This is the conclusion we eventually came to, and that is commonly admitted by other people working on this kind of system<sup><a href="#footnote_0_1033" id="identifier_0_1033" class="footnote-link footnote-identifier-link" title="Dronolab, a quadrotor project involving mechanical, electrical and software engineering students. They moved from a PID controller to a more sophisticated one. http://dronolab.etsmtl.ca/uav/">1</a></sup>. A solution has been researched to solve this difficulty, and a non-linear stabilization algorithm<sup><a href="#footnote_1_1033" id="identifier_1_1033" class="footnote-link footnote-identifier-link" title="Daniel Tabak, on a general digitally controlled system: An Algorithm for Nonlinear Process Stabilization and Control, 1970">2</a></sup>, adapted to the four rotors design, seems to be the more appropriate. Solutions have been designed, implemented and experimented successfully<sup><a href="#footnote_2_1033" id="identifier_2_1033" class="footnote-link footnote-identifier-link" title="A interesting thorough study on the design of an embedded control architecture for a four-rotors unmanned air vehicle to perform autonomous hover ﬂight:&nbsp;Escareno J., Salazar-Cruz S. and Lozano R, Embedded control of a four-rotor UAV, 2006">3</a></sup>, on similar platforms. This is a vast field of study in itself, and could still use more investigation. Unfortunately, this calls for some long dwelling on the mechanics of the quadricopter, and would require to have independent access to each of the motors (with the API, we can only control movements at a higher level, by choosing the pitch, roll and yaw), and preferably on the embedded firmware, both of which being as for now not made possible by Parrot who designed our ARDrone.</p>
<h3 style="text-align: left;">Patching our PID controller with an auto-hovering threshold</h3>
<p style="text-align: justify;">Once again, we wanted to find our way toward our project&#8217;s goal by investigating our own solutions. As we saw it during <a title="First experiment with auto-tracking" href="../../first-experiments/index.html" target="_blank">the first experiment we performed</a> at the beginning of our project, the embedded hovering function is pretty efficient in terms of stabilizing the drone. And stabilization is a feature we lack when our PID is reaching its zero-error point, since this is when it starts to &#8220;explode&#8221; by correcting parameters it should, due to small errors right on top of the target (our PID is nevertheless best at going right to the target, by adjusting itself progressively, without much overshoot, when the drone is further from its target).</p>
<p style="text-align: justify;">However, we cannot rely on this hovering feature alone to track one robot, not to mention a whole flock of them. As a consequence, we thought about the following compromise: auto-hovering (i.e. stabilized hovering performed by an embedded algorithm we do not control) would be automatically activated when the drone finds itself in a circular area on top of our point of interest (the robot leader), whereas our PID controller would take control over the motor when outside of this restricted area (cf. <strong>Figure 1</strong>).</p>
<table class="aligncenter" width="500" align="center">
<tbody>
<tr>
<td><a href="../wp-content/uploads/2011/08/hoverArea.png"><img class="aligncenter size-medium wp-image-1049" title="hoverArea" src="../wp-content/uploads/2011/08/hoverArea.png" alt="" width="500" height="440" srcset="../wp-content/uploads/2011/08/hoverArea.png 662w, ../wp-content/uploads/2011/08/hoverArea-300x263.png 300w, ../wp-content/uploads/2011/08/hoverArea-100x87.png 100w" sizes="(max-width: 500px) 100vw, 500px" /></a></td>
</tr>
<tr>
<th><strong>Figure 1</strong>: Illustration of the behavior of the drone. The green rectangle shows the 2D field of view (FOV) as it is projected on the ground for the vertical camera. As long as the error radius (distance between target and drone) is greater than a threshold, the drone is controlled by our PID regulator. Once it reaches this threshold, PID control is stopped and the embedded hovering algorithm takes over and tries to stabilize the quadricopter.</th>
</tr>
</tbody>
</table>
<p style="text-align: justify;">By combining the best of both designs, we did achieve our best stabilization so far. Then, tracking a moving robot is also a task well-performed, which now completely fulfills the purpose of our drone, when added to the flock coordinate-reporting process. Yet we really lacked a bigger test-room setting, where our main concern was the low ceiling, preventing a long-term efficient robot-tracking, since it is really hard to keep even one robot in the camera field of view when you cannot fly comfortably higher than an average of 1,8 meters. Well, at least, that put some stimulating additional challenge into our tasks.</p>
<h2 style="text-align: justify;">Drone and ground units: how our UAV is fully controlled</h2>
<p style="text-align: justify;">We have accomplished many tasks with our ARDrone, so it might not seem quite obvious what their purpose are in the scope of our project, to the point where one could wonder how we give it commands depending on which events. This part of our article focuses on a quick recap on all the implemented methods that enable the drone to do its required actions.</p>
<h3 style="text-align: justify;">Auto landing and taking-off</h3>
<p style="text-align: justify;">Landing and taking-off are the only two stages of a whole flight over which we cannot have any control beyond sending commands to start either of those maneuvers. Once the drone acknowledges the command, the embedded software takes over the controls and operates it. Our custom algorithm never triggers these operations, which are decided by the person in charge of the drone who uses a specifically programmed controller (see part below).</p>
<p style="text-align: justify;">Besides, safe landing may happen without our own input, in case of low battery.</p>
<h3 style="text-align: justify;">Manual control</h3>
<p style="text-align: justify;">Even though most of flying time is managed by our custom algorithm which takes control over commands of the drone, we still need to be able to enter some user input at some point. The drone is first of all a rather flimsy and even sometimes dangerous flying object that could deal some minor damage to itself our its surroundings in case of bad handling: it is therefore necessary to have at least the capacity to quickly stop it completely in an emergency, or better, to switch back to a full human control in case of unwanted behavior. Moreover, since our algorithms are a lot about tuning parameters, it is advisable to enable on-the-flight parameter tweaking. The most appropriate way appeared to use a game controller that we would map to take advantage of all the sticks (great for moving the drone) and the different buttons (to give more custom orders).</p>
<p style="text-align: justify;"><strong>Figure 2</strong> shows the XBox 360 we chose and how we map its buttons to call different functions in our program. The following sums up our gamepad functionalities:</p>
<ul style="text-align: justify;">
<li><strong>Start or stop landing or taking off</strong> phase. This can be done at any time.</li>
<li><strong>Emergency stop</strong>, enabled at any time, to suddenly cut all four motors. Sometimes also required before a new take off to reset the drone&#8217;s state.</li>
<li><strong>Flat trim</strong> tells the drone that it is currently in a horizontal position: necessary for proper landing, take off and hovering &#8211; and should be done before taking off.</li>
<li><strong>Yaw, altitude, pitch and roll</strong> represent all degrees of freedom of the drone, split on two sticks. If our custom algorithm is disabled, then it is possible to control all the drone&#8217;s movements into 3D space in real-time.</li>
<li><strong>Start or stop custom algorithm</strong> enables or disables our control algorithm that will try to locate and track a leader on the ground, while reporting coordinates of all units in the flock to the leader.</li>
<li><strong>Viewpoint</strong> changes the camera viewpoint displayed on the user screen (switching between vertical, horizontal, or both cameras). While custom algorithm is enabled, you want and need to use the vertical camera.</li>
<li><strong>Stop program</strong> stops pretty much everything, except manual control.</li>
<li><strong>Start/stop hovering</strong> disables custom algorithm and manual input to activate the embedded function that stabilizes the drone. It comes of great use when you just need a stable hovering drone that does not move much.</li>
<li>At last, you can select PID parameters, one after the other, and change them.</li>
</ul>
<p>&nbsp;</p>
<table class="aligncenter" width="500" align="center">
<tbody>
<tr>
<td><a href="../wp-content/uploads/2011/08/x360_controller_drone_transparent.png"><img class="aligncenter size-medium wp-image-1050" title="x360_controller_drone_transparent" src="../wp-content/uploads/2011/08/x360_controller_drone_transparent.png" alt="" width="500" height="338" srcset="../wp-content/uploads/2011/08/x360_controller_drone_transparent.png 1047w, ../wp-content/uploads/2011/08/x360_controller_drone_transparent-300x202.png 300w, ../wp-content/uploads/2011/08/x360_controller_drone_transparent-1024x692.png 1024w, ../wp-content/uploads/2011/08/x360_controller_drone_transparent-100x67.png 100w" sizes="(max-width: 500px) 100vw, 500px" /></a></td>
</tr>
<tr>
<th><strong>Figure 2</strong>: Finally, this is how our controller is mapped. We get with it an absolute control over all the possible actions of the drone in real-time, on top of lots of ease to tune our parameters during running time. Note that this should work with any other generic controller (at least remotely compatible with Linux).</th>
</tr>
</tbody>
</table>
<p>&nbsp;</p>
<h3 style="text-align: justify;">Algorithm control</h3>
<p style="text-align: justify;">By a simple button press, one can enable our disable our custom algorithm. But what exactly does it encompass ?</p>
<h4 style="text-align: justify;">Double PID</h4>
<p style="text-align: justify;">We have a double PID controller affecting three degrees of freedom, in order to stabilize two behaviors:</p>
<ul style="text-align: justify;">
<li>a constant altitude, that we want to be set (by default) around 1,8 meters. This is done through the <code>gaz</code> command.</li>
<li>tracking the leader, and hovering on top of it by detecting its tag. This is done through the <code>pitch</code> and <code>roll</code> commands.</li>
</ul>
<p style="text-align: justify;">More details are provided in the first part of this article and in previous ones: <a title="Performing simple image analysis and full PID controller with the Drone" href="../../performing-simple-image-analysis-and-full-pid-controller-with-the-drone/index.html" target="_blank">1</a>, <a title="Drone: new PID with polar coordinates and HowTo improve reactivity and accuracy" href="../../drone-new-pid-with-polar-coordinates-and-howto-improve-reactivity-and-accuracy/index.html" target="_blank">2</a>.</p>
<h4 style="text-align: justify;">Tilt handling</h4>
<p style="text-align: justify;">Coordinates of the target to follow are corrected depending on the inclination and the altitude of the drone, before being fed to the PID loop. This problem and its solution are thoroughly discussed <a title="Tracking algorithm: considering the inclination of the drone" href="../../tracking-algorithm-considering-the-inclination-of-the-drone/index.html" target="_blank">in a former article</a>.</p>
<h4 style="text-align: justify;">Detecting ground units and reporting their coordinates</h4>
<p style="text-align: justify;">Not only do we detect the tag that the drone has to track, but we also use <a title="Image analysis: color detection for multiple robots" href="../../image-analysis-color-detection-for-multiple-robots/index.html" target="_blank">our color object detection algorithm</a> to report all coordinates of the whole set of robots to the leader of the flock.</p>
<p style="text-align: justify;">This implies that it also handles a part of a network layer, while the program starts a thread that acts as a client connecting to the leader.</p>
<h4 style="text-align: justify;">Change of tracked leader</h4>
<p style="text-align: justify;">The very last feature we needed to carry out in order to fulfill our main goals was to make sure we are tracking the right robot. Since all the robots are supposed to follow the leader, and the leader being controlled by a human user, it seems appropriate to track the unit targeted by the other units.  This way, we make sure that the leader in always in sight, which enables establishing the rest of the formation accordingly. Furthermore, if a random unit were to get lost (out of the camera&#8217;s FOV), the leader, assisted by a human, could go looking for it; once it gets close to the lost unit, the latter becomes part of the formation again, since it is finally in the camera&#8217;s FOV.</p>
<p style="text-align: justify;">We then had to establish a protocol to decide who is going to be the leader, how the drone gets this information and how it should respond to it:</p>
<ol>
<li style="text-align: justify;">Leader number (<code>LeaderNumber</code>, i.e. the id of the robot chosen as a leader) is decided on the flock side, usually by the human who controls the leader itself.</li>
<li style="text-align: justify;"><code>LeaderNumber</code> is passed to the drone&#8217;s program through the same client/server socket connection that is used for sending the flock coordinates. The drone has a thread continuously listening for new events on this socket, and registers the current <code>LeaderNumber</code>.</li>
<li style="text-align: justify;">Once we get this <code>LeaderNumber</code>, the drone has to try to follow the corresponding unit on the ground. However, in some cases, the unit may not be in sight, or the <code>LeaderNumber</code> may not even be decided yet. The logical steps that help the drone take the right course of action is detailed in the decision tree in <strong>Figure 3</strong>.</li>
</ol>
<p>&nbsp;</p>
<table class="aligncenter" width="573" align="center">
<tbody>
<tr>
<td><a href="../wp-content/uploads/2011/08/LeaderNumber_DecisionTree_transparent.png"><img class="aligncenter size-full wp-image-1051" title="LeaderNumber_DecisionTree_transparent" src="../wp-content/uploads/2011/08/LeaderNumber_DecisionTree_transparent.png" alt="" width="573" height="477" srcset="../wp-content/uploads/2011/08/LeaderNumber_DecisionTree_transparent.png 819w, ../wp-content/uploads/2011/08/LeaderNumber_DecisionTree_transparent-300x249.png 300w, ../wp-content/uploads/2011/08/LeaderNumber_DecisionTree_transparent-100x83.png 100w" sizes="(max-width: 573px) 100vw, 573px" /></a></td>
</tr>
<tr>
<th><strong>Figure 3</strong>: Decision tree to find the number (id) of the robot leader and the drone&#8217;s action that should ensue. <em>Hover</em> means here simply hovering on the current spot, with the embedded stabilization algorithm.</th>
</tr>
</tbody>
</table>
<p style="text-align: justify;">
Basically, this means that we have added a new way to directly influence the behavior of the drone, by adding a input for the robot leader, on top of the human direct input with the gamepad and the algorithmic PID control.</p>
<h4>Testing everything together</h4>
<p style="text-align: justify;">Our very last tests include all the above-mentioned features, that appear to work quite smoothly simultaneously, even when working with our flock of four robots. No unexpected behaviors were observed during our final experiments, so everything was pretty much already discussed in previous articles. The last part that was asking for testing consists in the leader switching task. Since our robot detection, our communication protocol and our drone hovering were already performing good separately, we did not have much tuning to do. Video 1 below is here to illustrate this performance.</p>
<p>&nbsp;</p>
<table class="aligncenter" width="420" align="center">
<tbody>
<tr>
<td><object width="420" height="345" classid="clsid:d27cdb6e-ae6d-11cf-96b8-444553540000" codebase="http://download.macromedia.com/pub/shockwave/cabs/flash/swflash.cab#version=6,0,40,0"><param name="allowFullScreen" value="true" /><param name="allowscriptaccess" value="always" /><param name="src" value="http://www.youtube.com/v/_EKoG7ML7VU?version=3&amp;hl=en_US" /><param name="allowfullscreen" value="true" /><embed width="420" height="345" type="application/x-shockwave-flash" src="http://www.youtube.com/v/_EKoG7ML7VU?version=3&amp;hl=en_US" allowFullScreen="true" allowscriptaccess="always" allowfullscreen="true" /></object></td>
</tr>
<tr>
<th><strong>Video 1</strong>: The ARDrone is tracking two robots from our flock. Our flock-control program switches the &#8220;leader&#8221; (done manually here, for illustration purpose), i.e. the robot that leads the flock and that the drone is supposed to follow.So here, the leader switches back and forth between red and blue, and the drone moves accordingly and tries to hover on top of it.Hovering is not 100% steady. This is greatly due to a lack of altitude from the drone; our ceiling is too low to get a field of view good enough!For filming and testing purpose, it was necessary to focus on no more than two still units. Note that it however work as good as with more, moving robots.</th>
</tr>
</tbody>
</table>
<p>Videos and other more complete experiments will be to watch in our next article, where we deal with the whole flock and the drone.&nbsp;<br />
<b>References</b></p>
<ol class="footnotes">
<li id="footnote_0_1033" class="footnote"><em>Dronolab, a quadrotor project involving mechanical, electrical and software engineering students.</em> They moved from a PID controller to a more sophisticated one. <a href="http://dronolab.etsmtl.ca/uav/" onclick="__gaTracker('send', 'event', 'outbound-article', 'http://dronolab.etsmtl.ca/uav/', 'http://dronolab.etsmtl.ca/uav/');" target="_blank">http://dronolab.etsmtl.ca/uav/</a> [<a href="#identifier_0_1033" class="footnote-link footnote-back-link">&#8617;</a>]</li>
<li id="footnote_1_1033" class="footnote">Daniel Tabak, on a general digitally controlled system: <em>An Algorithm for Nonlinear Process Stabilization and Control, 1970</em> [<a href="#identifier_1_1033" class="footnote-link footnote-back-link">&#8617;</a>]</li>
<li id="footnote_2_1033" class="footnote">A interesting thorough study on the design of an embedded control architecture for a four-rotors unmanned air vehicle to perform autonomous hover ﬂight: <em>Escareno J., Salazar-Cruz S. and Lozano R, Embedded control of a four-rotor UAV, 2006</em> [<a href="#identifier_2_1033" class="footnote-link footnote-back-link">&#8617;</a>]</li>
</ol>
<div class='shareaholic-canvas' data-app-id='17416102' data-app='recommendations' data-title='Final words on the drone: merging previous work with new tasks' data-link='https://www.ludep.com/final-words-on-the-drone-merging-previous-work-with-new-tasks/' data-summary=''></div>		</div>
		<div class="clear"></div>
		
	</div>
		<div id="post-907" class="post post-907 type-post status-publish format-standard has-post-thumbnail hentry category-drone category-programming category-testing">
		<h2 class="h1"><a href="../../image-analysis-color-detection-for-multiple-robots/index.html" rel="bookmark">Image analysis: color detection for multiple robots</a></h2>

					<table class="info entry-meta">
				<tr>
					<td class="date">August 17, 2011</td>
					<td>
													<span class="postedby">
								Posted by <a href="http://michael/" title="Visit Michael&#8217;s website" rel="author external">Michael</a> 
							</span>
								<span class="filledunder">under</span>
												<span class="filledunder">
							<a href="../../category/drone/index.html" rel="category tag">Drone</a>, <a href="../../category/programming/index.html" rel="category tag">Programming</a>, <a href="../../category/testing/index.html" rel="category tag">Testing</a>						</span>
					</td>
											<td>
						
				
							<div class="act">
																	<span class="comments">Comments off</span>
																							</div>
						</td>
					
				</tr>
			</table>
				
									
		
		
		<div class="content">
			
			<div style="float:right;padding: 0 0 10px 10px" class="interactive_right"><iframe allowtransparency="true" frameborder="0" scrolling="no" src="http://platform.twitter.com/widgets/tweet_button.html?url=https%3A%2F%2Fwww.ludep.com%2Fimage-analysis-color-detection-for-multiple-robots%2F&amp;text=Image%20analysis:%20color%20detection%20for%20multiple%20robots&amp;count=vertical&amp;lang=" style="width:65px; height:65px;"></iframe><iframe src="http://www.facebook.com/plugins/like.php?href=https%3A%2F%2Fwww.ludep.com%2Fimage-analysis-color-detection-for-multiple-robots%2F&amp;layout=box_count&amp;show_faces=false&amp;width=65&amp;action=like&amp;font=arial&amp;colorscheme=light&amp;height=65" scrolling="no" frameborder="0" style="border:none; overflow:hidden; width:65px; height:65px;" allowTransparency="true"></iframe></div><h2 style="text-align: justify;">Introduction to our problem</h2>
<p style="text-align: justify;">Image analysis is nothing new in our project. We have already performed some by ourselves, using the vertical camera of the drone to track roundels on the ground. While this was a good solution, quite fast to implement and that did not required lots of tuning, it is no longer valid anymore when it comes to detecting more that one robot and differentiating all the units, since we were only considering a circular shape.</p>
<p style="text-align: justify;">We might have tried to identify the robots by the size of their circle marker that could be different according to the id of the robot. It works in a very well-defined environment (steady camera, not too many, too small nor too close roundels); yet the camera is moving, even vertically, so the size of one roundel is never constant (thus making an absolute size detection impossible &#8211; i.e. attributing one specific roundel size to a robot&#8217;s id) and the resolution of the camera not good enough for that purpose, compared to the precision of the altitude sensor that could have help correct the size estimation depending on the altitude. Finally, some robots may not be in the camera&#8217;s field of view from time to time (thus making a relative detection impossible &#8211; i.e. guessing a robot&#8217;s id by observing that its circle is bigger or smaller than the one of its neighbor). The low resolution of the camera plays an important part in what we can or cannot do: while it is good enough for localization purpose of one point, it is not sufficient enough to tell the difference between different small variations of size for multiple objects that are around 20 centimeters big &#8211; besides, the more the robots are on the field, the higher the drone needs to be in the sky, and the harder it gets to achieve such an analysis.</p>
<table class="aligncenter" width="420" align="center">
<tbody>
<tr>
<td><a href="../wp-content/uploads/2011/08/markers_closeup_edit.png"><img class="aligncenter size-medium wp-image-945" title="markers_closeup" src="../wp-content/uploads/2011/08/markers_closeup_edit-300x146.png" alt="" width="300" height="146" srcset="../wp-content/uploads/2011/08/markers_closeup_edit-300x146.png 300w, ../wp-content/uploads/2011/08/markers_closeup_edit-1024x498.png 1024w, ../wp-content/uploads/2011/08/markers_closeup_edit-100x48.png 100w" sizes="(max-width: 300px) 100vw, 300px" /></a></td>
</tr>
<tr>
<th style="text-align: justify;"><strong>Figure 1</strong>: Two of the markers installed on top of the omnidirectional robots. They are made of a brightly colored paper sheet folded into a polygon shape, with one angle providing a direction. Each shape is mounted on a LEGO support, of which one can notice the small &#8220;arms&#8221; going out of the polygons.</th>
</tr>
</tbody>
</table>
<p style="text-align: justify;">So the solution has to be different in terms of the type of object we want to detect. Furthermore this type of object also has to enable a differentiation between many robots (up to ten if we want to be realistic given the system we have). Using the robot itself as an object to be detected is not doable, since all of them have the same shape, and the differences that we could physically build on them would be too minor to be noticed from the sky with our low-resolution camera. Moreover, a robot is a complex object from a camera point of view, and it is harder to detect a complex object with irregularities, holes, reliefs that spawn shadows, etc. than a simpler, flat one. Like we previously did with the roundel, we need a marker for each robot. Figure 1 shows the kind of marker we put on our robots.</p>
<p style="text-align: justify;">We keep using the open source computer vision framework OpenCV to take advantage of its large number of already implemented algorithms and its C++ compatibility.</p>
<h3 style="text-align: justify;">Solutions for detecting a marker</h3>
<p style="text-align: justify;">OpenCV provides developers with hundreds of algorithms that are designed to solve many problems, and often a dozen of implemented techniques might be applied to approach the same problem in different ways. Computer vision being only a fraction of what we learned and used as computer science students and interns, we cannot pretend to have an extensive knowledge of most of the possible solutions to our situation, hence our perhaps limited below suggestions that had to fit within the scope of our project. Our goal is to get a workable result that proves a concept &#8211; once this is done, if it only works with a defined set of robots and markers because of our lack of practice with image analysis and OpenCV, then we know that it could be improved to be fully made all-purpose with more time spent on the mathematical side of the algorithms and their implementation.</p>
<ul>
<li style="text-align: justify;"><strong>Lukas-Kanade feature tracking algorithm</strong><sup><a href="#footnote_0_907" id="identifier_0_907" class="footnote-link footnote-identifier-link" title="Article by B. Lucas and T. Kanade, an iterative image registration technique with an application to stereo vision in Int. Joint Conference in Artificial Intelligence, pp. 674-680, 1981, describing the original feature point tracking algorithm.">1</a></sup> that exploits the fundamental optical flow constraint equation. This methods tracks features points as they move from frame to frame. It starts detecting a group of features points in an initial frame and then tries to find them in the next frame, and keep doing so while updating from time to time the feature points. While this may be a valid solution with a static camera where only the robots are moving, it will surely be harder if not impracticable with a moving camera and moving robots.</li>
</ul>
<ul>
<li style="text-align: justify;"><strong>Mixture of Gaussian method</strong><sup><a href="#footnote_1_907" id="identifier_1_907" class="footnote-link footnote-identifier-link" title="A complete description of the Mixture of Gaussian algorithm can be found in the article by C. Stauffer and W.E.L. Grimson, Adaptive background mixture models for real-time tracking, in Conf. on Computer Vision and Pattern Recognition, 1999.">2</a></sup>. It is basically an algorithm that extracts foreground objects in a scene while also coping with the problem of a moving background object (like tree leaves) or a glaring effect, thanks to sophisticated additions (e.g. the running variance is also considered on top of the running average for a model &#8211; so more data are kept and analyzed). This might be used in our case, even with a moving camera, but this would means that we make sure to keep flying and moving over a regular, flat, featureless and uni-color/pattern ground.</li>
</ul>
<ul>
<li style="text-align: justify;"><strong>Template matching method</strong>. It enables the detection of a specific pattern or part of a bigger image by looking for matches between the analyzed image and another, smaller image &#8211; the template, that contains the object we are trying to find. It does so by sliding the template over the image, going from the top left to the bottom right corner. The drawback of this solution is that is does not automatically take possible rotations of the object to match into account, nor does it consider the scaling of the object that changes depending on the altitude of the camera. Implementing those additional treatments would be possible but may be tedious and likely hardly efficient in terms of complexity (a naive algorithm would have to consider all the possible orientation over 360 degrees, on top of different template sizes&#8230;). Yet, if done correctly and providing a well thought template, this may become a really elegant solution, because it would rely on a bare minimum of external environmental factors.</li>
</ul>
<ul>
<li style="text-align: justify;"><strong>Color detection and tracking</strong> looks for specific colors in an image. This methods removes all the colors that are not wanted, so as to only keep the color we need to locate, by applying different color filters on each image. This is frame-independent, i.e. it does not require to keep a trace in memory of what happened before, which is a positive aspect since frames may have almost nothing in common depending on the movements of the drone. This is however a surrounding-light sensitive solution, which means that results will vary depending on the daylight and lights that are turned on. So tuning might be often required before a new experiment.</li>
</ul>
<h3>Chosen technique</h3>
<p style="text-align: justify;">We have chosen tracking by color object detection, since this is a solution we knew could be implemented quickly with fair results. It may not be the best because of its important drawbacks, but our setup is such that those are mostly insignificant: we work in an office where light conditions greatly depend on ceiling lights, so we can keep a nearly constant lighting environment at all time. Besides, we do not need to track more than a few couples of robots, which mean that we can choose quite easily colors sharply contrasting with each other. Yet we are aware that this is no sustainable solution if we were to implement it on a greater scale in a different environment like outdoors, but image analysis is only one of our many concerns in this project, not our focus &#8211; so we could not afford to spend six months working on this topic alone.</p>
<p style="text-align: justify;">On top of that, we also learnt from other fellow students from Aarhus University, who faced a similar problem with an &#8220;eye-in-the-sky&#8221; static camera tracking two ground robots in real-time. They investigated many possible solutions and eventually disregarded all of them to choose the color detection. Yet they paid close attention to some of them, by even selecting first the Haar Classifier Cascades, that they did implement. However, this is an algorithm based on Machine Learning that requires a lot of &#8220;training&#8221; time, and performed badly in their setup, with unworkable results. Since our working environment is almost the same as theirs, with the same kind of hardware that is visually complex (LEGO studs are seen during the training of the machine with clear, clean pictures, but the video stream is much more noisy, and in our case even distorted by the inclination of the drone &#8211; so the same pattern on a picture may not be recognized in a video), and with the same software library OpenCV, it made a lot of sense to follow their track and try to build on what has been already done. All the details are available in their labreport<sup><a href="#footnote_2_907" id="identifier_2_907" class="footnote-link footnote-identifier-link" title="Hammer Slammer LEGO Game - Harald Andertun, Tom Oersnes Thorgaard, Mark Surrow:&nbsp;http://legologbook.blogspot.com/">3</a></sup>.</p>
<p style="text-align: justify;">This academic work, a recipe book for OpenCV<sup><a href="#footnote_3_907" id="identifier_3_907" class="footnote-link footnote-identifier-link" title="OpenCV 2 Computer Vision Application Programming Cookbook&nbsp;(Paperback) by Robert Laganiere, Packt Publishing Limited -&nbsp;ISBN 13: 9781849513241&nbsp;ISBN 10: 1849513244">4</a></sup> and lessons from our former professor of image analysis<sup><a href="#footnote_4_907" id="identifier_4_907" class="footnote-link footnote-identifier-link" title="David Roussel, research professor working at the IBISC laboratory (http://www.lami.univ-evry.fr/) ">5</a></sup> provided us with plenty of inspiring ideas and practical ways to implement them.</p>
<h2 style="text-align: justify;">Color object tracking</h2>
<h3 style="text-align: justify;">Method and implementation</h3>
<p style="text-align: justify;"><em>As an aside, please keep in mind that our following explanation may be best understood by looking into our commented source code at the same time, available in our </em><a title="Links and Downloads" href="../../links-downloads/index.html" target="_blank">Link &amp; Downloads</a><em> section. The most relevant file for the problem we analyze here is </em><code>ColorMatcher.cpp </code><em>(in folder </em><code>drone_app_code_sdk_1.7/application/Source/ColorMatching</code><em>).</em></p>
<p style="text-align: justify;"><em></em> Tracking a given object usually requires to proceed in two major distinct steps:</p>
<ol>
<li>Identify the object, by producing a binary image showing where certain objects of interest could be located. This can for instance be done by histogram back projection or by motion analysis.</li>
<li>Extract the objects contained in this binary collection, i.e. extract the connected components (shapes made of a set of connected pixels in a binary image).</li>
</ol>
<p style="text-align: justify;">Then it is a good practice habit to display some graphical information on the video stream so the user may visualize how the algorithm performs.</p>
<p style="text-align: justify;">Analyzing a video signal is pretty much similar to analyzing a single picture. Video signals are made of a sequence of images called frames, that are taken at a regular pace, the frame rate. Once the individual frames of a video have been extracted, methods to analyze them are no different from those applied to pictures, and the results are the same, as long as we do not overdo too much heavy processing to keep up with the frame rate.</p>
<p style="text-align: justify;">So for the remaining of our explanation, we do as though we deal with a single frame, given that we then repeat the same process forever (as long as the drone is running). Note that each frame is temporarily copied into another memory location: the copy will be analyzed by our algorithm, while the original will have some graphical information drawn on top of it every time we go through one loop for one color and we identify a color. If this is not done, then the drawing interferes with the detection algorithm, since it is then part of the frame. The original frame is shown to the user, while the copy is deleted once all the color have been tracked on the current frame.</p>
<h4 style="text-align: justify;">Identifying the colors</h4>
<p style="text-align: justify;">We try here to track the markers presented on Figure 1. Since we are only dealing with color matching in a first time, we pay no attention on any other characteristic of the marker (neither the shape nor the orientation are relevant).</p>
<p style="text-align: justify;">Basically, when the program is launched, it is initialized with some static configuration that provides the parameters needed to identify each robot. To keep the matter simple, we therefore have one robot associated with one range of color values, in a one to one relationship. During our further explanations, we will deal with only one robot/color,  since the process is repeated in the same way for every other color (the difference being the values of the parameters passed to our function).</p>
<p style="text-align: justify;">For one color, we create a temporary color mask that is going to be our binary image required for the next step. It first starts as an empty data structure of the same dimension as the original frame.</p>
<pre>IplImage* colorMask = cvCreateImage(
                        cvSize(
                          frame-&gt;width,
                          frame-&gt;height
                        ),
                        8, // image type (8 bits image)
                        1 // one single channel
                      );</pre>
<p style="text-align: justify;">Then we apply the following method on our frame:</p>
<pre>cvInRangeS(frameCopy,
           cvScalar(robot-&gt;B_val_min, robot-&gt;R_val_min, robot-&gt;G_val_min),
           cvScalar(robot-&gt;B_val_max, robot-&gt;R_val_max, robot-&gt;G_val_max),
           colorMask
);</pre>
<p style="text-align: justify;"><code>cvInRangeS</code> extracts all the pixels that are in the color range defined by the second and third parameters [lower and upper bound of the interval] and gives them a binary value of 1 that will be stored in the <code>colorMask</code> matrix, while all the other filtered-out pixel get a 0 value. A color is defined here by its Red, Green, Blue (RGB) composition, thus we need to specify three value for each bound of the interval. Therefore, one can assert that a robot is completely defined by 6 values.</p>
<h4 style="text-align: justify;">Extracting the objects</h4>
<p style="text-align: justify;">From now on we have our binary data structure filled, which we can use. Another temporary data structure is needed, that is going to hold a vector of contours. Initialization is done as follows:</p>
<pre>CvMemStorage* storage = cvCreateMemStorage();</pre>
<p style="text-align: justify;">OpenCV offers a simple function to extract the contours of the connected components of an image:</p>
<pre>int numberOfContours = cvFindContours(colorMask,      // our binary image from step 1.
                                      storage,        // a vector of contours
                                      &amp;first_contour  // a pointer to the first contour found
);</pre>
<p style="text-align: justify;">The input is our binary image previously obtained. The output is a vector of contours, with each contour being represented by a vector of <code>CvPoint</code>. The extraction is performed by an algorithm that systematically scans the image until a component is found. Then, it follows its contour from this starting point while marking the pixel on its border. Once the contour is fully found, the scanning resumes at the last position until it finds a new component.</p>
<p style="text-align: justify;">Then, we filter out some of the components we are sure we want to be eliminated, using some prior knowledge about the expected size of the object of interest. We therefore discard all the components whose bounding box is smaller than a defined size. A bounding box is the most compact way to represent a component in an image, defined as the upright rectangle of minimum size that contains the shape entirely. It cannot provide more information than the location of the object and an approximation of its size. That is the easiest structure to use for us, and is created in our code like this:</p>
<pre>cvRect bound = cvBoundingRect(storage[0], 0)</pre>
<p style="text-align: justify;">However, filtering all the small bounding boxes revealed to be insufficient to get eventually one unique box that could be identified as the wanted color/robot, because of some unexpected light noise that might appear on one frame or another. Hence our decision to only keep the biggest one (the noise is usually not covering a surface bigger than the robot itself).</p>
<h4 style="text-align: justify;">Displaying the objects</h4>
<p style="text-align: justify;">Finally, we draw all the bounding boxes we found on our original frame, except for those that are smaller than our threshold, as it displayed on Figure 2. Then we add a marker (a circle and a tag name) on what we consider to be the actual robot, that is, the biggest rectangle (cf. video 1 or Figure 3 for an illustration).</p>
<p style="text-align: justify;">Once our analyze is done for one frame and one color, we obviously do not forget to reset and release the temporarily allocated memory chunk to avoid a memory overflow that can happen really quickly when it comes to deal with a video signal:</p>
<pre>cvClearMemStorage(storage);
cvReleaseMemStorage(&amp;storage);
first_contour = NULL;
cvReleaseImage(&amp;colorMask);</pre>
<p style="text-align: justify;">We also draw a direction vector that make averages on the last ten registered positions for each robot, so we can get a new information on the overall direction of a unit.</p>
<p style="text-align: justify;">Obviously, while drawing our bounding boxes, we also update the corresponding robot structure with its newly found coordinates. We convert the matrix coordinates into real measurements by applying the same calculations that we already did with the drone and one roundel<sup><a href="#footnote_5_907" id="identifier_5_907" class="footnote-link footnote-identifier-link" title="Tracking algorithm: considering the inclination of the drone, Situation Modeling: http://www.ludep.com/tracking-algorithm-considering-the-inclination-of-the-drone">6</a></sup> (it basically consists in multiplying our value by a MAPPING_SCALE parameter that depends on the altitude of the camera and its Field Of View angle). Automatically, the new position of the robot is sent to the server that manages the flock.</p>
<p>&nbsp;</p>
<table class="aligncenter" width="534" align="center">
<tbody>
<tr>
<td><a href="../wp-content/uploads/2011/08/MultiBlob_4colors_cropped.png"><img class="aligncenter size-full wp-image-904" title="MultiBlob_4colors_cropped" src="../wp-content/uploads/2011/08/MultiBlob_4colors_cropped.png" alt="" width="534" height="494" srcset="../wp-content/uploads/2011/08/MultiBlob_4colors_cropped.png 835w, ../wp-content/uploads/2011/08/MultiBlob_4colors_cropped-300x277.png 300w, ../wp-content/uploads/2011/08/MultiBlob_4colors_cropped-100x92.png 100w" sizes="(max-width: 534px) 100vw, 534px" /></a></td>
</tr>
<tr>
<th style="text-align: justify;"><strong>Figure 2</strong>: Detection of multiple objects at the same time. This is our main interface that enables us to test and see what happens in real-time, by providing tools to tune parameters &#8220;on the go&#8221;.  The left window is the user input interface, the top right window displays the video with a matching colors overlay, and the small bottom right window currently displays the extracted contours corresponding to our red color.</th>
</tr>
</tbody>
</table>
<p>&nbsp;</p>
<p style="text-align: justify;">The Graphical User Interface provides the user with some liberty in adjusting the parameters in real-time:</p>
<ul>
<li style="text-align: justify;"><strong>Field edge size</strong> is the size of the virtual border all-around the image that triggers a specific signal once a robot is in its boundaries. This is later used by the server and the leader that will try to prevent robots to escape the field of view as often as possible.</li>
<li style="text-align: justify;"><strong>Cam distance</strong> is the distance from the camera to the ground. This is actually only used with a webcam on the ceiling (see below), and is updated automatically when the drone is used (thanks to its sensors).</li>
<li style="text-align: justify;"><strong><strong>Position filter threshold</strong> </strong>is the minimum distance that has to be between two components before considering them as two different objects.</li>
<li style="text-align: justify;"><strong>Bound min size</strong> is the minimum size of one side of a bounding box so that its inner component is considered as a potential robot.</li>
<li style="text-align: justify;"><strong>Smoothing (Gaussian)</strong> applies a filter that blurs the image. This is a convenient way of getting rid of noise inside frames that has otherwise a tendency to produce lots of small, scarce components. It is thus practically always kept active.</li>
<li style="text-align: justify;"><strong>Require convex shapes</strong> can be activated to disregards all the components that are not convex. Given the lack of absolute precision in the detection, it is rare that components are always perfectly enclosed object, so this is probably too selective a parameter.</li>
<li style="text-align: justify;"><strong>Socket I/O</strong> enables or disables communication with a server. <em>(more about that feature in our project&#8217;s report)</em></li>
<li style="text-align: justify;"><strong>Video</strong> opens a window that displays the video signal in real-time, with the graphical overlay.</li>
<li style="text-align: justify;"><strong>Log</strong> is used for debugging: if activated, it prints debug and information messages in real-time into the console.</li>
<li style="text-align: justify;"><strong>PanelR#</strong> opens or closes the settings window for a given robot. This new panel allows the user to change separately one of the 6 color parameters (i.e. the RGB interval boundaries), in real-time. It also enables another video view where is solely shown the computed contours (the <code>storage</code> vector) for the associated color.</li>
</ul>
<h3>Experiments</h3>
<h4>One steady webcam setup</h4>
<p style="text-align: justify;">In order to not over complicate the testing of this part of our project, we decided to first test our recognition algorithm with an USB webcam attached to the ceiling and facing down toward our testing field, before connecting it to our drone and dealing with the hassle of flying it altogether. We also ran a simple server locally which task was merely to print out the messages -containing the robots coordinates- sent by our detection class.</p>
<p style="text-align: justify;">This webcam has a higher resolution of 640*480 pixels than the vertical one embedded in the drone (176 * 144 pixels). This entails a higher accuracy in what was observed, but also a major slowdown in the processing time. Actually, the laptop we use for this purpose is getting old and is not able in itself to just correctly display the video stream at a good frame rate and lag-free. While this was not an issue to test the basic mechanics of our algorithm, and to see that the color detection was efficient enough for our purpose, it quickly became a problem when we introduced communication with the robots, since they were not able to react to their current state, and rather moved accordingly to already obsolete results.</p>
<p style="text-align: justify;">That is the reason why we later thought about an intermediate solution that consists in hanging the drone on the ceiling by the means of strings, and then establish a WiFi connection with it without starting its motors, in order to get its video signal. This enabled us to tune our algorithm even better and approach our final system step by step. Besides, what we lost in resolution was gained in terms of frame rate, computing time and greater field of view (64 degrees instead of 52 degrees, which is a nice improvement given our rather low ceiling).</p>
<p style="text-align: justify;">We therefore created a class that can be instantiated for both configurations, depending on the available setup and current needs. This <code>ColorMatcher</code> class handle then all the color detection algorithm, server communication and Graphical User Interface (GUI) by itself. The following piece of code is enough to launch all the threads that manage those tasks:</p>
<pre>ColorMatcher* colorMatcher = new ColorMatcher(true // use the webcam rather than the drone's camera
                                              true // start by displaying the video signal (GUI adjustable)
IplImage* frame;

while ( (frame = colorMatcher-&gt;getCurrentFrame()) != NULL)
{
    colorMatcher-&gt;analyzeFrame(frame);
}</pre>
<h4>One robot</h4>
<p style="text-align: justify;">Starting with one robot, i.e. one color, was a good way to learn how to tune our parameters. A good start is to identify the RGB code of the tag we consider by comparing it to samples in an online database if necessary, then to create a first interval of minimum and maximum boundaries by substracting or adding a constant to those RGB values. Then, watching the color mask evolving in real-time on the screen enables a quick and efficient tuning, usually by tweaking only one or two more parameters, using sliders of the GUI.</p>
<p style="text-align: justify;">Results went better than expected because of the really good detection speed and the fairly stable performances we observed (cf. Video 1). Surely, if we were to turn off at least one light in the room would the results be different with plenty of noise and false positives. Light sensitivity is the obvious drawback of this system.</p>
<table class="aligncenter" width="420" align="center">
<tbody>
<tr>
<td><object width="420" height="345" classid="clsid:d27cdb6e-ae6d-11cf-96b8-444553540000" codebase="http://download.macromedia.com/pub/shockwave/cabs/flash/swflash.cab#version=6,0,40,0"><param name="allowFullScreen" value="true" /><param name="allowscriptaccess" value="always" /><param name="src" value="http://www.youtube.com/v/R2yafY4n3QE?version=3&amp;hl=en_US" /><param name="allowfullscreen" value="true" /><embed width="420" height="345" type="application/x-shockwave-flash" src="http://www.youtube.com/v/R2yafY4n3QE?version=3&amp;hl=en_US" allowFullScreen="true" allowscriptaccess="always" allowfullscreen="true" /></object></td>
</tr>
<tr>
<th>
<p style="text-align: justify;"><strong>Video 1:</strong> The robot&#8217;s behavior has no special meaning &#8211; it&#8217;s merely a test with a still webcam ofgood resolution that is used to detect specific colors on the ground. The control window on the right (mostly black) shows what happens on the algorithm side when we want to detect the red color.</p>
<p style="text-align: justify;">Coordinates and values returned by the algorithm (for further used by the ground robots) are displayed in the console on the bottom right.</p>
</th>
</tr>
</tbody>
</table>
<p>&nbsp;</p>
<h4><code>N</code> robots</h4>
<p style="text-align: justify;">Going from one robot to <code>n</code> robots has being made an easy task in our class. At any time, one just needs to add the following line into the program to tell it that it should now also watch for a new range of color and handle a new robot.</p>
<pre>colorMatcher-&gt;addRobot(B_val_min, G_val_min, R_val_min, B_val_max, G_val_max, R_val_max);</pre>
<p style="text-align: justify;">The more the robots we have, the harder it is to find colors that can be easily distinguished if we want to deal with slight lighting variations. Still, we did not have any issue with four colors (the highest number we could test here, due to the material limitation we have with the number of robots we could possibly build), as it is shown in Figure 3.</p>
<p style="text-align: justify;">The complexity of our algorithm being nothing more than linear (<code>O(n)</code>)  in terms of the number of robots and not quadratic or worse, we did not expect, nor did we observe, a longer computational time and a performance loss. Actually, the algorithm keeps being executed faster than the frame rate. Video 2 is here to illustrate the performance of our color detection program for multiple robots.</p>
<table class="aligncenter" width="720" align="center">
<tbody>
<tr>
<td style="vertical-align: middle;"><a href="../wp-content/uploads/2011/08/Flock_4Robots_cropped.png"><img class="aligncenter size-medium wp-image-951" title="Flock_4Robots" src="../wp-content/uploads/2011/08/Flock_4Robots_cropped-300x267.png" alt="" width="300" height="267" srcset="../wp-content/uploads/2011/08/Flock_4Robots_cropped-300x267.png 300w, ../wp-content/uploads/2011/08/Flock_4Robots_cropped-100x89.png 100w, ../wp-content/uploads/2011/08/Flock_4Robots_cropped.png 675w" sizes="(max-width: 300px) 100vw, 300px" /></a></td>
<td><object width="420" height="345" classid="clsid:d27cdb6e-ae6d-11cf-96b8-444553540000" codebase="http://download.macromedia.com/pub/shockwave/cabs/flash/swflash.cab#version=6,0,40,0"><param name="allowFullScreen" value="true" /><param name="allowscriptaccess" value="always" /><param name="src" value="http://www.youtube.com/v/inOph-wTeh4?version=3&amp;hl=en_US" /><param name="allowfullscreen" value="true" /><embed width="420" height="345" type="application/x-shockwave-flash" src="http://www.youtube.com/v/inOph-wTeh4?version=3&amp;hl=en_US" allowFullScreen="true" allowscriptaccess="always" allowfullscreen="true" /></object></td>
</tr>
<tr>
<th style="text-align: justify;"><strong>Figure 3</strong>: Detection of four objects at the same time. Notice how the resolution has changed compared to video 1, due to a switch from a webcam to the drone&#8217;s camera. This is also the best field of view we can hope to have in our testing room: the drone is about 270 cm high, providing a FOV of around 180*220 cm².</th>
<th style="text-align: justify;"><strong>Video 2</strong>: Detection of four objects in real-time. The color location is quite steady on a whole run. Still, this video illustrates some consequences of non precise tuning, where Robot#4 (yellow) is once mistaken for Robot#1 (red), which can lead to wrong behaviors.Observe also (solved) issues with green detection: some parts of the floor are sometimes seen as green, but since they are always small in size, the algorithm never confuses them for Robot#3 &#8211; as long as it is on the field.</th>
</tr>
</tbody>
</table>
<p>&nbsp;</p>
<h4>Experimenting with different surroundings</h4>
<p style="text-align: justify;">We have tried our whole system with the drone&#8217;s camera and four robots in another environment (our LEGOlab, Zuze building, in Aarhus), where the place is well-lit by the daylight and the floor is mostly a blue carpet. Results were poor compared to what we get in our own office, that consists in a light gray floor and is mostly influenced by indoors lights. Basically, the blue carpet triggered a bad detection quality by greatly decreasing  the contrast with our own markers. Unsurprisingly, the red one was still quite well detected, while all the other colors appeared almost white on the screen. That is due to the camera that overcompensates the relative prevailing darkness of the blue carpet by making anything else much brighter in contrast, to the point where it is hard to tell the difference between our already bright, blue, green and yellow colors. It was really obvious once we get a white shirt of one of us appearing in the field of view: after a couple of seconds of adaptation, the detection was back to being nearly perfect again, whereas the blue carpet was still prevalent. This is explained by the fact that this shirt provided a new element of sharp contrast with the remaining of the image, which was then compensated the other way around by the camera. The ambient bright daylight was also not here to help it, because of the somewhat reflective surface of our tags and the noise generated on the video signal.</p>
<h3>Results with the drone</h3>
<h4>Tests</h4>
<p style="text-align: justify;">Dealing with a moving drone is supposed to complicate the detection for the worse. Surprisingly, once the color parameters are tuned, the detection keeps being smooth &#8211; and nothing is slow down (neither the tracking algorithm nor the drone&#8217;s control algorithm).</p>
<p style="text-align: justify;">The increase or decrease of distance from the camera to the field did not have any noticeable effect on our detection, as long as the drone does not go beyond an altitude of around four meters, in which case it would require bigger tags (or a higher-resolution camera). Besides, quick movements and large inclinations of the drone were not affecting our tracking. We also feared that the shadow cast by the drone itself on the ground would cause visible issues, but none was to be reported. Coordinates sent were still consistent in the own relative camera coordinates system, which helped efficiently the robots to position themselves on the field <em>(more on this topic in another article, and lots of example videos of the whole system in our Youtube channel)</em>.</p>
<h4 style="text-align: justify;">Improvements</h4>
<p style="text-align: justify;">One improvement that may be however considered to add more flexibility to the use of our drone, by making it less light-dependent, may lie in the polygonal approximation of a component&#8217;s contour, instead of its rough bounding box. This way, we could detect shapes instead of colors -like one regular polygon for one robot-, while still keeping almost all our algorithm. Color detection would still be used but with a much broader and permissive interval, insofar as we would not have to differentiate the robot&#8217;s colors between each other, and contour detection would also be applied. We would then need to change a few lines of code while changing the bounding rectangles to a creation of a new polygon, which would be close to this:</p>
<pre>std::vector poly;  // the polygon we want to create from the contour
cvApproxPolyDP(cv::Mat(storage[0]),
               poly,
               5,      // accuracy of the approximation
               true    // yes it is a closed shape
);</pre>
<p style="text-align: justify;">And after that, we could identify the polygon by counting the number of its sides. NB: the convex hull is another form of polygonal approximation that may also be worth considering.</p>
<p style="text-align: justify;">We have already tried the component detection with a broader color interval, and it revealed to be really efficient in the task of differentiating the robots from the other elements on the ground, most likely because of the bright colors that were used as markers. We however lacked time to implement the remaining of the polygon detection (and it was not also absolutely necessary since we achieved our main goal without it), but we believe it to be a sustainable solution, even though we are still not fully assured of its success, because of the low resolution of the camera that would certainly make this impossible if the objects were too small.</p>
<p style="text-align: justify;">A possible alternative would be a mix between color detection and template matching, by tracking the robot using template matching, Kalman filtering and color-histogram back-projection. Templates of robots would still be extracted from our color mask using connected component extraction. New locations would be predicted by a Kalman filter. The template matching would be a nice added feature to our algorithm, since it would then provide it with more flexibility and a self calibrating process. This method was used to track soccer players and a ball in a football game, that can be applied in real-time with a good efficiency<sup><a href="#footnote_6_907" id="identifier_6_907" class="footnote-link footnote-identifier-link" title="Sunghoon Choi, Yongduek Seo, Hyunwoo Kim, Ki-Sang Hong,&nbsp;Where are the ball and players?: Soccer game analysis with color-based tracking and image mosaik">7</a></sup>. This is a situation quite similar to our issue, where we need to keep track of different elements belonging to the same team and moving on a two-dimensional field.</p>
<p>&nbsp;<br />
<b>References</b></p>
<ol class="footnotes">
<li id="footnote_0_907" class="footnote">Article by <em>B. Lucas and T. Kanade, an iterative image registration technique with an application to stereo vision in Int. Joint Conference in Artificial Intelligence, pp. 674-680, 1981</em>, describing the original feature point tracking algorithm. [<a href="#identifier_0_907" class="footnote-link footnote-back-link">&#8617;</a>]</li>
<li id="footnote_1_907" class="footnote">A complete description of the Mixture of Gaussian algorithm can be found in the article by <em>C. Stauffer and W.E.L. Grimson, Adaptive background mixture models for real-time tracking, in Conf. on Computer Vision and Pattern Recognition, 1999</em>. [<a href="#identifier_1_907" class="footnote-link footnote-back-link">&#8617;</a>]</li>
<li id="footnote_2_907" class="footnote">Hammer Slammer LEGO Game &#8211; Harald Andertun, Tom Oersnes Thorgaard, Mark Surrow: <a href="http://legologbook.blogspot.com/" onclick="__gaTracker('send', 'event', 'outbound-article', 'http://legologbook.blogspot.com/', 'http://legologbook.blogspot.com/');">http://legologbook.blogspot.com/</a> [<a href="#identifier_2_907" class="footnote-link footnote-back-link">&#8617;</a>]</li>
<li id="footnote_3_907" class="footnote"><em>OpenCV 2 Computer Vision Application Programming Cookbook</em> (Paperback) by Robert Laganiere, Packt Publishing Limited &#8211;<strong> ISBN 13</strong>: 9781849513241 <strong>ISBN 10</strong>: 1849513244 [<a href="#identifier_3_907" class="footnote-link footnote-back-link">&#8617;</a>]</li>
<li id="footnote_4_907" class="footnote">David Roussel, research professor working at the IBISC laboratory (<a href="http://www.lami.univ-evry.fr/" onclick="__gaTracker('send', 'event', 'outbound-article', 'http://www.lami.univ-evry.fr/', 'http://www.lami.univ-evry.fr/');">http://www.lami.univ-evry.fr/</a>)  [<a href="#identifier_4_907" class="footnote-link footnote-back-link">&#8617;</a>]</li>
<li id="footnote_5_907" class="footnote">Tracking algorithm: considering the inclination of the drone, Situation Modeling: <a href="../../tracking-algorithm-considering-the-inclination-of-the-drone/index.html" target="_blank">http://www.ludep.com/tracking-algorithm-considering-the-inclination-of-the-drone</a> [<a href="#identifier_5_907" class="footnote-link footnote-back-link">&#8617;</a>]</li>
<li id="footnote_6_907" class="footnote"><em>Sunghoon Choi, Yongduek Seo, Hyunwoo Kim, Ki-Sang Hong, <a href="http://www.google.dk/url?sa=t&amp;source=web&amp;cd=1&amp;ved=0CBQQFjAA&amp;url=http%3A%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fdownload%3Fdoi%3D10.1.1.31.3849%26rep%3Drep1%26type%3Dpdf&amp;ei=an5STr2IMo7CswaprNmLAw&amp;usg=AFQjCNFWRNwnT9LPb6gk-KQy3IU2pSP0OA&amp;sig2=PyvYmiBQR3yIYACUUV93wg" onclick="__gaTracker('send', 'event', 'outbound-article', 'http://www.google.dk/url?sa=t&amp;source=web&amp;cd=1&amp;ved=0CBQQFjAA&amp;url=http%3A%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fdownload%3Fdoi%3D10.1.1.31.3849%26rep%3Drep1%26type%3Dpdf&amp;ei=an5STr2IMo7CswaprNmLAw&amp;usg=AFQjCNFWRNwnT9LPb6gk-KQy3IU2pSP0OA&amp;sig2=PyvYmiBQR3yIYACUUV93wg', 'Where are the ball and players?: Soccer game analysis with color-based tracking and image mosaik');" title="Article (pdf)"  target="_blank">Where are the ball and players?: Soccer game analysis with color-based tracking and image mosaik</a></em> [<a href="#identifier_6_907" class="footnote-link footnote-back-link">&#8617;</a>]</li>
</ol>
<div class='shareaholic-canvas' data-app-id='17416102' data-app='recommendations' data-title='Image analysis: color detection for multiple robots' data-link='https://www.ludep.com/image-analysis-color-detection-for-multiple-robots/' data-summary=''></div>		</div>
		<div class="clear"></div>
		
	</div>
		<div id="post-841" class="post post-841 type-post status-publish format-standard has-post-thumbnail hentry category-ground-robots category-ideas category-programming category-refining-the-project category-testing">
		<h2 class="h1"><a href="../../merging-our-work-together-the-beginnings-of-worthy-and-notable-results/index.html" rel="bookmark">Merging our work together: the beginnings of worthy and notable results.</a></h2>

					<table class="info entry-meta">
				<tr>
					<td class="date">August 4, 2011</td>
					<td>
													<span class="postedby">
								Posted by <a href="../../index.html" title="Visit Guillaume&#8217;s website" rel="author external">Guillaume</a> 
							</span>
								<span class="filledunder">under</span>
												<span class="filledunder">
							<a href="../../category/ground-robots/index.html" rel="category tag">Ground robots</a>, <a href="../../category/ideas/index.html" rel="category tag">Ideas</a>, <a href="../../category/programming/index.html" rel="category tag">Programming</a>, <a href="../../category/refining-the-project/index.html" rel="category tag">Refining the project</a>, <a href="../../category/testing/index.html" rel="category tag">Testing</a>						</span>
					</td>
											<td>
						
				
							<div class="act">
																	<span class="comments">Comments off</span>
																							</div>
						</td>
					
				</tr>
			</table>
				
									
		
		
		<div class="content">
			
			<div style="float:right;padding: 0 0 10px 10px" class="interactive_right"><iframe allowtransparency="true" frameborder="0" scrolling="no" src="http://platform.twitter.com/widgets/tweet_button.html?url=https%3A%2F%2Fwww.ludep.com%2Fmerging-our-work-together-the-beginnings-of-worthy-and-notable-results%2F&amp;text=Merging%20our%20work%20together:%20the%20beginnings%20of%20worthy%20and%20notable%20results.&amp;count=vertical&amp;lang=" style="width:65px; height:65px;"></iframe><iframe src="http://www.facebook.com/plugins/like.php?href=https%3A%2F%2Fwww.ludep.com%2Fmerging-our-work-together-the-beginnings-of-worthy-and-notable-results%2F&amp;layout=box_count&amp;show_faces=false&amp;width=65&amp;action=like&amp;font=arial&amp;colorscheme=light&amp;height=65" scrolling="no" frameborder="0" style="border:none; overflow:hidden; width:65px; height:65px;" allowTransparency="true"></iframe></div><p style="text-align: justify;">It&#8217;s been two weeks now we have been working together in order to merge our work (one on the drone/image analysis and the other on the land units/flock behavior). This post will present you our latest changes, improvements and show you the first results we came out with.</p>
<p>&nbsp;</p>
<h2 style="text-align: justify;">More about the flock behavior</h2>
<p style="text-align: justify;">The flock behavior is not entirely implemented yet. By &#8220;entirely&#8221; and &#8220;yet&#8221;, we mean that we only have two robots: one leading and the other one following, being as such the only following member of the flock. Still, the flock behavior is on its way to the full implementation and we have right now a lot of features that make the system properly working. Among those features, you can namely find:</p>
<p>&nbsp;</p>
<h3 style="text-align: justify;">An enhanced GUI</h3>
<p style="text-align: justify;">We wanted to be able to choose the bricks we wanted to use, be able to reconnect them if any problem happened, without being forced to restart all the program and the bricks. We came out with the following design, handy for taking control according to our needs and to display the information we needed from the bricks.</p>
<p style="text-align: justify;"><a href="../wp-content/uploads/2011/08/GUI1.png"><img class="aligncenter size-full wp-image-842" title="GUI1" src="../wp-content/uploads/2011/08/GUI1.png" alt="" width="900" height="105" srcset="../wp-content/uploads/2011/08/GUI1.png 900w, ../wp-content/uploads/2011/08/GUI1-300x35.png 300w, ../wp-content/uploads/2011/08/GUI1-100x11.png 100w" sizes="(max-width: 900px) 100vw, 900px" /></a></p>
<p>&nbsp;</p>
<p style="text-align: justify;">First notable thing is that the GUI is dynamic: it will display a number of lines according to the devices initially added in the program. For each device, you can connect it in order to add it to the flock simply by clicking on its button or clicking on the &#8220;Connect all bricks&#8221; button.</p>
<p>&nbsp;</p>
<p style="text-align: justify;"><a href="../wp-content/uploads/2011/08/GUI2.png"><img class="aligncenter size-full wp-image-846" title="GUI2" src="../wp-content/uploads/2011/08/GUI2.png" alt="" width="901" height="106" srcset="../wp-content/uploads/2011/08/GUI2.png 901w, ../wp-content/uploads/2011/08/GUI2-300x35.png 300w, ../wp-content/uploads/2011/08/GUI2-100x11.png 100w" sizes="(max-width: 901px) 100vw, 901px" /></a></p>
<p>&nbsp;</p>
<p style="text-align: justify;">Once connected, the second button will allow to take control of the brick either using the controller, or writing in the field with command lines compatible with the brick&#8217;s interpreter. The brick will feed the computer back in real-time: confirmation message on the first label and the battery level on the second one (we will talk more about this one later).</p>
<p>&nbsp;</p>
<h3 style="text-align: justify;">The change of leader</h3>
<p style="text-align: justify;">The change of leader is something we wanted to focus on because no one never knows what problem is going to happen during an experiment and for instance, the leader might perish. So, at this point, it was important to know what strategy to adopt if the flock loses its leader. The solution we used is simply to change randomly the leader if this one is lost. Nevertheless, another solution can be added: we could indeed find the nearest robot to the leader and give it the lead. Another solution would be to give different priorities to the robots as if they had more importance according to their status (it is easy to state that a tank is more likely to take the lead rather than an ambulance carrying people).</p>
<p style="text-align: justify;">As we mentioned before, it is possible to change the leader of the flock and thus change the formation in real-time. The bricks in use are checked quite often (in a separate thread); so even if a brick connects to the flock after the program has started or disconnects from it during the experiment, a new leader will be picked, automatically or manually. Mentioning that, the notion of disconnection brought us to develop an interesting feature that follows.</p>
<p>&nbsp;</p>
<h3 style="text-align: justify;">The management of dead units</h3>
<p style="text-align: justify;">If we want to compute the positions of every brick, we need to know the exact number of units in the flock and if they are properly connected. If not, we would have a lot of delay and this would tremendously affect our results. The brick must warn the computer of its state so as to ease the computer in its calculation.</p>
<p style="text-align: justify;">As soon as a brick is connected, it has a thread running, giving every half second a heartbeat. On the computer side, we make sure for each brick that we receive those heartbeats. If not, after five heartbeats missed (2.5 seconds of silence from the brick), we shut the connection and get ready to re-open it whenever the user wants it (maybe the time to fix the brick and put it back on track). At the same time, the brick itself sends its own battery level in every heartbeat: this is an information that can be relevantly used by the user and depending on a certain threshold (that we set around 6,1V), the brick sends a message <em>&#8220;Battery level too low&#8221;</em>, shuts the connection with the computer and turn itself down.</p>
<p style="text-align: justify;">Here is a sample of the computer-side code for the heartbeat counter checker.</p>
<pre>public void run()
	{
		resetCount();

		while (getHeartBeatChecker() &lt; 5) {
			try {
				Thread.sleep(500);
			} catch (InterruptedException e) {
				e.printStackTrace();
			}

			setHeartBeatChecker(getHeartBeatChecker()+1);
		}

		brick.closeConnection();
		interrupt();
	}</pre>
<p>&nbsp;</p>
<h3 style="text-align: justify;">An oriented flock</h3>
<p style="text-align: justify;">This is the feature we are the most proud of and where the omniwheels are the most useful and worthy. The flock is not just supposed to follow the leader in a simple direction: no matter which direction the leader is going to, the flock is going to stay behind. Thus, the flock is moving really often, especially when the leader is changing direction, but this is even more pleasant to watch when it happens (this will be completely covered in the next and final article about the flock behavior, this is just a sample/teaser).</p>
<table class="aligncenter" width="546">
<tbody>
<tr>
<td><a href="../wp-content/uploads/2011/08/fb1.png"><img class="aligncenter size-medium wp-image-862" title="fb1" src="../wp-content/uploads/2011/08/fb1-253x300.png" alt="" width="253" height="300" srcset="../wp-content/uploads/2011/08/fb1-253x300.png 253w, ../wp-content/uploads/2011/08/fb1-84x100.png 84w, ../wp-content/uploads/2011/08/fb1.png 477w" sizes="(max-width: 253px) 100vw, 253px" /></a></td>
<td><a href="../wp-content/uploads/2011/08/fb2.png"><img class="aligncenter size-medium wp-image-863" title="fb2" src="../wp-content/uploads/2011/08/fb2-253x300.png" alt="" width="253" height="300" srcset="../wp-content/uploads/2011/08/fb2-253x300.png 253w, ../wp-content/uploads/2011/08/fb2-84x100.png 84w, ../wp-content/uploads/2011/08/fb2.png 477w" sizes="(max-width: 253px) 100vw, 253px" /></a></td>
</tr>
<tr>
<th>As soon as the leader changes direction&#8230;</th>
<th>&#8230;the flock adapts and changes its orientation</th>
</tr>
</tbody>
</table>
<p>&nbsp;</p>
<h3 style="text-align: justify;">An enclosure system</h3>
<p style="text-align: justify;">The field of view of the camera can be a major problem in our project. For instance, a robot can be asked to go out of sight so as to respect the position it is supposed to be at. This is not a suitable behavior (indeed, we want to keep all the units in the flock) so every time a robot is close to the limit of the FOV, it is asked to get closer to the leader. The drone is supposed to be on top of the leader, so the units will get closer to the camera&#8217;s FOV doing such.</p>
<p>&nbsp;</p>
<h2 style="text-align: justify;">What the flock looks like so far</h2>
<h3 style="text-align: justify;">Very first working test</h3>
<p style="text-align: justify;">On this video, the flock is just supposed to stay below the leader (blue unit) and thus does not take care of the orientation given by the user/leader: the flock is supposed to stay oriented towards the white wall at the background.</p>
<table class="aligncenter" width="425">
<tbody>
<tr>
<td>
<p style="text-align: justify;"><object width="425" height="349" classid="clsid:d27cdb6e-ae6d-11cf-96b8-444553540000" codebase="http://download.macromedia.com/pub/shockwave/cabs/flash/swflash.cab#version=6,0,40,0"><param name="allowFullScreen" value="true" /><param name="allowscriptaccess" value="always" /><param name="src" value="http://www.youtube.com/v/BQfWkJomMmw?version=3&amp;hl=en_US" /><param name="allowfullscreen" value="true" /><embed width="425" height="349" type="application/x-shockwave-flash" src="http://www.youtube.com/v/BQfWkJomMmw?version=3&amp;hl=en_US" allowFullScreen="true" allowscriptaccess="always" allowfullscreen="true" /></object></p>
</td>
</tr>
</tbody>
</table>
<p>The behavior is not suitable at all, the robot stops and starts over all the time: we&#8217;re not getting the fluidity we were looking for. This is why we implemented a P (proportional) correction on the movement and it fixed the problem as you will see it in the next videos. Nevertheless, a first step was made: the flock was moving as expected and it was ready to go to the step further.</p>
<p>&nbsp;</p>
<h3 style="text-align: justify;">Testing the oriented flock</h3>
<p style="text-align: justify;">From now on, the flock is oriented, no matter what direction the user sets (the red unit follows the blue one, once more). Nonetheless, we didn&#8217;t implement a collision avoidance layer yet, so you might want to be careful not to change the orientation and driving forward another robot for instance.</p>
<table class="aligncenter" width="425">
<tbody>
<tr>
<td>
<p style="text-align: justify;"><object width="425" height="349" classid="clsid:d27cdb6e-ae6d-11cf-96b8-444553540000" codebase="http://download.macromedia.com/pub/shockwave/cabs/flash/swflash.cab#version=6,0,40,0"><param name="allowFullScreen" value="true" /><param name="allowscriptaccess" value="always" /><param name="src" value="http://www.youtube.com/v/WrCJn1y8b8Y?version=3&amp;hl=en_US" /><param name="allowfullscreen" value="true" /><embed width="425" height="349" type="application/x-shockwave-flash" src="http://www.youtube.com/v/WrCJn1y8b8Y?version=3&amp;hl=en_US" allowFullScreen="true" allowscriptaccess="always" allowfullscreen="true" /></object></p>
</td>
</tr>
</tbody>
</table>
<p>&nbsp;</p>
<p>The robot behaves as expected. It even seems to reproduce the movements of the leader with a little delay but the flock is only positioning itself at the opposite direction of the leader. The P (proportional) correction is working quite well: we don&#8217;t have any overshot and it seems to be sufficient for what the robot are supposed to do.</p>
<h3 style="text-align: justify;">Taking a closer look to the &#8220;edge limit&#8221; feature</h3>
<p style="text-align: justify;">The only thing you have to know here is that the black stripes on the floor are the limits of the field of view. From this point, the behavior is pretty simple: every time the a robot is soon to be out of the FOV, it is asked to get closer to the leader.</p>
<table class="aligncenter" width="425">
<tbody>
<tr>
<td><object width="425" height="349" classid="clsid:d27cdb6e-ae6d-11cf-96b8-444553540000" codebase="http://download.macromedia.com/pub/shockwave/cabs/flash/swflash.cab#version=6,0,40,0"><param name="allowFullScreen" value="true" /><param name="allowscriptaccess" value="always" /><param name="src" value="http://www.youtube.com/v/XMUpBj0YynU?version=3&amp;hl=en_US" /><param name="allowfullscreen" value="true" /><embed width="425" height="349" type="application/x-shockwave-flash" src="http://www.youtube.com/v/XMUpBj0YynU?version=3&amp;hl=en_US" allowFullScreen="true" allowscriptaccess="always" allowfullscreen="true" /></object></td>
</tr>
</tbody>
</table>
<p style="text-align: justify;">The system is properly working. When the leader is no longer seen on the image (that shouldn&#8217;t happen because the drone is supposed to stay over the leader, but this is just for testing purposes), the robot following gets closer to the last recorded position of the leader (which was along the limits of the FOV)  and therefore doesn&#8217;t go out the FOV.</p>
<p>At this point, it is interesting to observe the whole behavior and get surprised with things we didn&#8217;t expect. For instance, when the leader is out of sight, the robot following gets slower and sometimes move to unpredictable positions (still along the FOV, and it might be due to some misinterpretation of the color recognition) and seems to be lost without its leader. As soon as the leader comes back in the field; the proportional correction implemented on the movements make the robot almost run towards the leader. Call that loyalty or something else, but it&#8217;s always nice to see that basic behaviors merged together can unexpectedly lead to real ones&#8230;</p>
<div class='shareaholic-canvas' data-app-id='17416102' data-app='recommendations' data-title='Merging our work together: the beginnings of worthy and notable results.' data-link='https://www.ludep.com/merging-our-work-together-the-beginnings-of-worthy-and-notable-results/' data-summary=''></div>		</div>
		<div class="clear"></div>
		
	</div>
<!--
		<div class="post">
			 <h2 id="post-841" class="h1"><a href="https://www.ludep.com/merging-our-work-together-the-beginnings-of-worthy-and-notable-results/" rel="bookmark">Merging our work together: the beginnings of worthy and notable results.</a></h2>
			
							<table class="info entry-meta">
				<tr>
					<td class="date">August 4, 2011</td>
					<td>
													<span class="postedby">
								Posted by <a href="http://www.ludep.com" title="Visit Guillaume&#8217;s website" rel="author external">Guillaume</a> 
							</span>
								<span class="filledunder">under</span>
												<span class="filledunder">
							<a href="https://www.ludep.com/category/ground-robots/" rel="category tag">Ground robots</a>, <a href="https://www.ludep.com/category/ideas/" rel="category tag">Ideas</a>, <a href="https://www.ludep.com/category/programming/" rel="category tag">Programming</a>, <a href="https://www.ludep.com/category/refining-the-project/" rel="category tag">Refining the project</a>, <a href="https://www.ludep.com/category/testing/" rel="category tag">Testing</a>						</span>
					</td>
											<td>
						
				
							<div class="act">
																	<span class="comments">Comments off</span>
																							</div>
						</td>
					
				</tr>
			</table>
						
			<div class="storycontent">
				<div style="float:right;padding: 0 0 10px 10px" class="interactive_right"><iframe allowtransparency="true" frameborder="0" scrolling="no" src="http://platform.twitter.com/widgets/tweet_button.html?url=https%3A%2F%2Fwww.ludep.com%2Fmerging-our-work-together-the-beginnings-of-worthy-and-notable-results%2F&amp;text=Merging our work together: the beginnings of worthy and notable results.&amp;count=vertical&amp;lang=  " style="width:65px; height:65px;"></iframe><iframe src="http://www.facebook.com/plugins/like.php?href=https%3A%2F%2Fwww.ludep.com%2Fmerging-our-work-together-the-beginnings-of-worthy-and-notable-results%2F&amp;layout=box_count&amp;show_faces=false&amp;width=65&amp;action=like&amp;font=arial&amp;colorscheme=light&amp;height=65" scrolling="no" frameborder="0" style="border:none; overflow:hidden; width:65px; height:65px;" allowTransparency="true"></iframe></div><p style="text-align: justify;">It&#8217;s been two weeks now we have been working together in order to merge our work (one on the drone/image analysis and the other on the land units/flock behavior). This post will present you our latest changes, improvements and show you the first results we came out with.</p>
<p>&nbsp;</p>
<h2 style="text-align: justify;">More about the flock behavior</h2>
<p style="text-align: justify;">The flock behavior is not entirely implemented yet. By &#8220;entirely&#8221; and &#8220;yet&#8221;, we mean that we only have two robots: one leading and the other one following, being as such the only following member of the flock. Still, the flock behavior is on its way to the full implementation and we have right now a lot of features that make the system properly working. Among those features, you can namely find:</p>
<p>&nbsp;</p>
<h3 style="text-align: justify;">An enhanced GUI</h3>
<p style="text-align: justify;">We wanted to be able to choose the bricks we wanted to use, be able to reconnect them if any problem happened, without being forced to restart all the program and the bricks. We came out with the following design, handy for taking control according to our needs and to display the information we needed from the bricks.</p>
<p style="text-align: justify;"><a href="http://www.ludep.com/wp-content/uploads/2011/08/GUI1.png"><img class="aligncenter size-full wp-image-842" title="GUI1" src="http://www.ludep.com/wp-content/uploads/2011/08/GUI1.png" alt="" width="900" height="105" srcset="../wp-content/uploads/2011/08/GUI1.png 900w, ../wp-content/uploads/2011/08/GUI1-300x35.png 300w, ../wp-content/uploads/2011/08/GUI1-100x11.png 100w" sizes="(max-width: 900px) 100vw, 900px" /></a></p>
<p>&nbsp;</p>
<p style="text-align: justify;">First notable thing is that the GUI is dynamic: it will display a number of lines according to the devices initially added in the program. For each device, you can connect it in order to add it to the flock simply by clicking on its button or clicking on the &#8220;Connect all bricks&#8221; button.</p>
<p>&nbsp;</p>
<p style="text-align: justify;"><a href="http://www.ludep.com/wp-content/uploads/2011/08/GUI2.png"><img class="aligncenter size-full wp-image-846" title="GUI2" src="http://www.ludep.com/wp-content/uploads/2011/08/GUI2.png" alt="" width="901" height="106" srcset="../wp-content/uploads/2011/08/GUI2.png 901w, ../wp-content/uploads/2011/08/GUI2-300x35.png 300w, ../wp-content/uploads/2011/08/GUI2-100x11.png 100w" sizes="(max-width: 901px) 100vw, 901px" /></a></p>
<p>&nbsp;</p>
<p style="text-align: justify;">Once connected, the second button will allow to take control of the brick either using the controller, or writing in the field with command lines compatible with the brick&#8217;s interpreter. The brick will feed the computer back in real-time: confirmation message on the first label and the battery level on the second one (we will talk more about this one later).</p>
<p>&nbsp;</p>
<h3 style="text-align: justify;">The change of leader</h3>
<p style="text-align: justify;">The change of leader is something we wanted to focus on because no one never knows what problem is going to happen during an experiment and for instance, the leader might perish. So, at this point, it was important to know what strategy to adopt if the flock loses its leader. The solution we used is simply to change randomly the leader if this one is lost. Nevertheless, another solution can be added: we could indeed find the nearest robot to the leader and give it the lead. Another solution would be to give different priorities to the robots as if they had more importance according to their status (it is easy to state that a tank is more likely to take the lead rather than an ambulance carrying people).</p>
<p style="text-align: justify;">As we mentioned before, it is possible to change the leader of the flock and thus change the formation in real-time. The bricks in use are checked quite often (in a separate thread); so even if a brick connects to the flock after the program has started or disconnects from it during the experiment, a new leader will be picked, automatically or manually. Mentioning that, the notion of disconnection brought us to develop an interesting feature that follows.</p>
<p>&nbsp;</p>
<h3 style="text-align: justify;">The management of dead units</h3>
<p style="text-align: justify;">If we want to compute the positions of every brick, we need to know the exact number of units in the flock and if they are properly connected. If not, we would have a lot of delay and this would tremendously affect our results. The brick must warn the computer of its state so as to ease the computer in its calculation.</p>
<p style="text-align: justify;">As soon as a brick is connected, it has a thread running, giving every half second a heartbeat. On the computer side, we make sure for each brick that we receive those heartbeats. If not, after five heartbeats missed (2.5 seconds of silence from the brick), we shut the connection and get ready to re-open it whenever the user wants it (maybe the time to fix the brick and put it back on track). At the same time, the brick itself sends its own battery level in every heartbeat: this is an information that can be relevantly used by the user and depending on a certain threshold (that we set around 6,1V), the brick sends a message <em>&#8220;Battery level too low&#8221;</em>, shuts the connection with the computer and turn itself down.</p>
<p style="text-align: justify;">Here is a sample of the computer-side code for the heartbeat counter checker.</p>
<pre>public void run()
	{
		resetCount();

		while (getHeartBeatChecker() &lt; 5) {
			try {
				Thread.sleep(500);
			} catch (InterruptedException e) {
				e.printStackTrace();
			}

			setHeartBeatChecker(getHeartBeatChecker()+1);
		}

		brick.closeConnection();
		interrupt();
	}</pre>
<p>&nbsp;</p>
<h3 style="text-align: justify;">An oriented flock</h3>
<p style="text-align: justify;">This is the feature we are the most proud of and where the omniwheels are the most useful and worthy. The flock is not just supposed to follow the leader in a simple direction: no matter which direction the leader is going to, the flock is going to stay behind. Thus, the flock is moving really often, especially when the leader is changing direction, but this is even more pleasant to watch when it happens (this will be completely covered in the next and final article about the flock behavior, this is just a sample/teaser).</p>
<table class="aligncenter" width="546">
<tbody>
<tr>
<td><a href="http://www.ludep.com/wp-content/uploads/2011/08/fb1.png"><img class="aligncenter size-medium wp-image-862" title="fb1" src="http://www.ludep.com/wp-content/uploads/2011/08/fb1-253x300.png" alt="" width="253" height="300" srcset="../wp-content/uploads/2011/08/fb1-253x300.png 253w, ../wp-content/uploads/2011/08/fb1-84x100.png 84w, ../wp-content/uploads/2011/08/fb1.png 477w" sizes="(max-width: 253px) 100vw, 253px" /></a></td>
<td><a href="http://www.ludep.com/wp-content/uploads/2011/08/fb2.png"><img class="aligncenter size-medium wp-image-863" title="fb2" src="http://www.ludep.com/wp-content/uploads/2011/08/fb2-253x300.png" alt="" width="253" height="300" srcset="../wp-content/uploads/2011/08/fb2-253x300.png 253w, ../wp-content/uploads/2011/08/fb2-84x100.png 84w, ../wp-content/uploads/2011/08/fb2.png 477w" sizes="(max-width: 253px) 100vw, 253px" /></a></td>
</tr>
<tr>
<th>As soon as the leader changes direction&#8230;</th>
<th>&#8230;the flock adapts and changes its orientation</th>
</tr>
</tbody>
</table>
<p>&nbsp;</p>
<h3 style="text-align: justify;">An enclosure system</h3>
<p style="text-align: justify;">The field of view of the camera can be a major problem in our project. For instance, a robot can be asked to go out of sight so as to respect the position it is supposed to be at. This is not a suitable behavior (indeed, we want to keep all the units in the flock) so every time a robot is close to the limit of the FOV, it is asked to get closer to the leader. The drone is supposed to be on top of the leader, so the units will get closer to the camera&#8217;s FOV doing such.</p>
<p>&nbsp;</p>
<h2 style="text-align: justify;">What the flock looks like so far</h2>
<h3 style="text-align: justify;">Very first working test</h3>
<p style="text-align: justify;">On this video, the flock is just supposed to stay below the leader (blue unit) and thus does not take care of the orientation given by the user/leader: the flock is supposed to stay oriented towards the white wall at the background.</p>
<table class="aligncenter" width="425">
<tbody>
<tr>
<td>
<p style="text-align: justify;"><object width="425" height="349" classid="clsid:d27cdb6e-ae6d-11cf-96b8-444553540000" codebase="http://download.macromedia.com/pub/shockwave/cabs/flash/swflash.cab#version=6,0,40,0"><param name="allowFullScreen" value="true" /><param name="allowscriptaccess" value="always" /><param name="src" value="http://www.youtube.com/v/BQfWkJomMmw?version=3&amp;hl=en_US" /><param name="allowfullscreen" value="true" /><embed width="425" height="349" type="application/x-shockwave-flash" src="http://www.youtube.com/v/BQfWkJomMmw?version=3&amp;hl=en_US" allowFullScreen="true" allowscriptaccess="always" allowfullscreen="true" /></object></p>
</td>
</tr>
</tbody>
</table>
<p>The behavior is not suitable at all, the robot stops and starts over all the time: we&#8217;re not getting the fluidity we were looking for. This is why we implemented a P (proportional) correction on the movement and it fixed the problem as you will see it in the next videos. Nevertheless, a first step was made: the flock was moving as expected and it was ready to go to the step further.</p>
<p>&nbsp;</p>
<h3 style="text-align: justify;">Testing the oriented flock</h3>
<p style="text-align: justify;">From now on, the flock is oriented, no matter what direction the user sets (the red unit follows the blue one, once more). Nonetheless, we didn&#8217;t implement a collision avoidance layer yet, so you might want to be careful not to change the orientation and driving forward another robot for instance.</p>
<table class="aligncenter" width="425">
<tbody>
<tr>
<td>
<p style="text-align: justify;"><object width="425" height="349" classid="clsid:d27cdb6e-ae6d-11cf-96b8-444553540000" codebase="http://download.macromedia.com/pub/shockwave/cabs/flash/swflash.cab#version=6,0,40,0"><param name="allowFullScreen" value="true" /><param name="allowscriptaccess" value="always" /><param name="src" value="http://www.youtube.com/v/WrCJn1y8b8Y?version=3&amp;hl=en_US" /><param name="allowfullscreen" value="true" /><embed width="425" height="349" type="application/x-shockwave-flash" src="http://www.youtube.com/v/WrCJn1y8b8Y?version=3&amp;hl=en_US" allowFullScreen="true" allowscriptaccess="always" allowfullscreen="true" /></object></p>
</td>
</tr>
</tbody>
</table>
<p>&nbsp;</p>
<p>The robot behaves as expected. It even seems to reproduce the movements of the leader with a little delay but the flock is only positioning itself at the opposite direction of the leader. The P (proportional) correction is working quite well: we don&#8217;t have any overshot and it seems to be sufficient for what the robot are supposed to do.</p>
<h3 style="text-align: justify;">Taking a closer look to the &#8220;edge limit&#8221; feature</h3>
<p style="text-align: justify;">The only thing you have to know here is that the black stripes on the floor are the limits of the field of view. From this point, the behavior is pretty simple: every time the a robot is soon to be out of the FOV, it is asked to get closer to the leader.</p>
<table class="aligncenter" width="425">
<tbody>
<tr>
<td><object width="425" height="349" classid="clsid:d27cdb6e-ae6d-11cf-96b8-444553540000" codebase="http://download.macromedia.com/pub/shockwave/cabs/flash/swflash.cab#version=6,0,40,0"><param name="allowFullScreen" value="true" /><param name="allowscriptaccess" value="always" /><param name="src" value="http://www.youtube.com/v/XMUpBj0YynU?version=3&amp;hl=en_US" /><param name="allowfullscreen" value="true" /><embed width="425" height="349" type="application/x-shockwave-flash" src="http://www.youtube.com/v/XMUpBj0YynU?version=3&amp;hl=en_US" allowFullScreen="true" allowscriptaccess="always" allowfullscreen="true" /></object></td>
</tr>
</tbody>
</table>
<p style="text-align: justify;">The system is properly working. When the leader is no longer seen on the image (that shouldn&#8217;t happen because the drone is supposed to stay over the leader, but this is just for testing purposes), the robot following gets closer to the last recorded position of the leader (which was along the limits of the FOV)  and therefore doesn&#8217;t go out the FOV.</p>
<p>At this point, it is interesting to observe the whole behavior and get surprised with things we didn&#8217;t expect. For instance, when the leader is out of sight, the robot following gets slower and sometimes move to unpredictable positions (still along the FOV, and it might be due to some misinterpretation of the color recognition) and seems to be lost without its leader. As soon as the leader comes back in the field; the proportional correction implemented on the movements make the robot almost run towards the leader. Call that loyalty or something else, but it&#8217;s always nice to see that basic behaviors merged together can unexpectedly lead to real ones&#8230;</p>
<div class='shareaholic-canvas' data-app-id='17416102' data-app='recommendations' data-title='Merging our work together: the beginnings of worthy and notable results.' data-link='https://www.ludep.com/merging-our-work-together-the-beginnings-of-worthy-and-notable-results/' data-summary=''></div>			</div>
			
		</div>
-->
		
		
		<div class="clear"></div>
				</div>
	</div>
</div>


<div class="clear"></div>
<div id="outer-footer">
<div id="footer">

<div id="blue-footer">
	<div class="footer-column">
		
	</div>
</div>
<div class="clear"></div>
</div> <!-- footer -->
</div> <!-- outer-footer -->
<div id="copyright" style="margin:5px; text-align:center; float:left;">
<b>LUDEP</b> powered by <a href="http://wordpress.org/">WordPress</a> and 
<a href="http://vatuma.com/">The Clear Line Theme</a><br/><br/>
</div>
<div style="float:right;">
	<div id="follow-widget-3" style="float:left;"><ul class="followwrap size24 row"><li class="icon_text"><a rel="nofollow me" target="_blank"  style="background: transparent url(../wp-content/plugins/share-and-follow/default/24/facebook.png) no-repeat top left;padding-left:28px;line-height:28px;" class="facebook" href="https://www.facebook.com/pages/Ludep/205635196128367" title="Become a fan"><span class="head">Become a fan</span></a></li><li class="icon_text"><a rel="nofollow me" target="_blank"  style="background: transparent url(../wp-content/plugins/share-and-follow/default/24/rss.png) no-repeat top left;padding-left:28px;line-height:28px;" class="rss" href="../../feed/atom/index.html" title="RSS/Atom feed"><span class="head">RSS/Atom feed</span></a></li><li class="icon_text"><a rel="nofollow me" target="_blank"  style="background: transparent url(../wp-content/plugins/share-and-follow/default/24/twitter.png) no-repeat top left;padding-left:28px;line-height:28px;" class="twitter" href="http://www.twitter.com/LUDEPdotCOM" title="Follow us"><span class="head">Follow us</span></a></li><li class="icon_text"><a rel="nofollow me" target="_blank"  style="background: transparent url(../wp-content/plugins/share-and-follow/default/24/youtube.png) no-repeat top left;padding-left:28px;line-height:28px;" class="youtube" href="http://www.youtube.com/LUDEPdotCOM" title="Youtube channel"><span class="head">Youtube channel</span></a></li></ul><div class="clean"></div> </div>	<div class="clear"></div>
</div>
<div class="clear"></div>
	<input type="hidden" id="social_connect_login_form_uri" value="https://www.ludep.com/wp-login.php" />
	<script type='text/javascript' src='../../wp-includes/js/wp-embed.min9738.js?ver=5.1.8'></script>

		</div> <!--container -->
</body>

<!-- Mirrored from www.ludep.com/2011/08/ by HTTrack Website Copier/3.x [XR&CO'2014], Mon, 15 Mar 2021 00:22:33 GMT -->
</html>